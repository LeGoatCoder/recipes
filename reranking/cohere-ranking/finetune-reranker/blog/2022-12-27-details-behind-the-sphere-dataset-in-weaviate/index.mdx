---
title: The Details Behind the Sphere Dataset in Weaviate
slug: details-behind-the-sphere-dataset-in-weaviate
authors: [zain]
date: 2022-12-27
tags: ['engineering', 'concepts']
image: ./img/hero.png
description: "Learn about the hardware, software and performance metric specifications behind our ~1B object import of the Sphere dataset into Weaviate."
---

![Sphere dataset in Weaviate](./img/hero.png)

# Introduction
In this post, we will go behind the scenes to show you how we got all of the data from the Sphere dataset into Weaviate. We will cover the hardware and software setup, as well as the performance metrics behind the import process.

# The Setup

## Hardware
To set up our cluster, we provisioned 6 Node GKE’s (Google Kubernetes Engine) in US Central1 Region (Zone C). Each node was an instance of type m1-ultramem-40, which has 40 vCPU’s and 961 GiB of RAM. In total, our setup had 240 vCPUs and 5766 GiB of memory.

## Software

### Weaviate
We deployed 6 instances of Weaviate (one on each GKE node) with automatic sharding per node for a total of 6 shards. Each Weaviate instance had 1800 GiB of disk memory and the `text2vec-huggingface` vectorizer module enabled. The Weaviate version used was built from v1.16.0 with a preview of the Hybrid Search functionality.

### Using the Spark Connector
To import the dataset into Weaviate, we used the [Spark Connector](https://github.com/weaviate/weaviate-spark-connector). The massive size of this dataset led to the use of the Spark dataframe, and the Spark connector can efficiently populate Weaviate from a Spark dataframe.

We used a single 64 GiB Google Cloud Platform(GCP) n2 instance to run Spark. The steps below can be followed to import data from the Sphere dataset.

# Importing the Sphere dataset into Weaviate

First, using the `pyspark` library, you can instantiate a connection, called a `SparkSession`:




Following this you need to import the Sphere dataset into a Spark dataframe, by loading the Parquet files that we have made available:




Next up, spin up a Weaviate instance (or cluster), connect to it and create a schema for the Sphere dataset:




Then import the contents of the Spark dataframe into Weaviate:




When this is done, your instance of Weaviate should have finished ingesting the specified data from Spark and be ready to go.

# Performance Metrics

In this section, we will take a look at a few stats related to the Sphere import run.

## Import run
The entire process took approximately 48 hours and the system performed very well. The graph beneath the text show the number of vectors imported over time:

![Number of vectors imported](./img/number-of-vectors-imported.png)

As can be seen the slope, indicating import speed, is almost linear.

## LSM stores
The object store stores the objects and their vectors, and the inverted index powers BM25+filter searches. The inverted index is not strictly required, but is mainly there to provide efficient filtering and more than worth the storage space it takes up. The LSM store size is well into terabytes of data:

![LSM stores](./img/LSM-stores.png)

## Batch imports
We used 22 parallel batches each with a batch size of 100. An average batch duration of around 1s was great to work with since the batches are large enough for Weaviate to receive enough data to process without it being a bottleneck, yet small enough to minimize the risk of timing out.

![Batch objects latency](./img/batch-objects-latency.png)

# Conclusion
Here we provided all the details of the hardware setup, the usage of the Spark connector and system performance during the Sphere dataset import run.

<StayConnected />

