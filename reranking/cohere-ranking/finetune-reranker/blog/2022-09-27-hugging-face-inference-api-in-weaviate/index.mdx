---
title: Support for Hugging Face Inference API in Weaviate
slug: hugging-face-inference-api-in-weaviate
authors: [sebastian]
date: 2022-09-27
tags: ['integrations']
image: ./img/hero.png
description: "Running ML Model Inference in production is hard. You can use Weaviate – a vector database – with Hugging Face Inference module to delegate the heavy lifting."
---
![Support for Hugging Face Inference API in Weaviate](./img/hero.png)

<h2 id="vector-databases-and-model-inference">Vector databases and model inference</h2>

Vector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data – text, image, audio, etc. – into vectors and use that for context-based queries) and many more use cases.

All of these use cases require `Machine Learning model inference` – a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) – which is a compute-heavy process.

<h2 id="the-elephant-in-the-room">The elephant in the room</h2>
Running model inference in production is hard.
<ul>
<li>It requires expensive specialized hardware.</li>
<li>You need a lot more computing power during the initial data import.</li>
<li>Hardware tends to be underutilized once the bulk of the heavy work is done.</li>
<li>Sharing and prioritizing resources with other teams is hard.</li>
</ul>

The good news is, there are companies – like Hugging Face, OpenAI, and Cohere – that offer running model inference as a service.

<blockquote>
<p>"Running model inference in production is hard,<br>
let them do it for you."</p>
</blockquote>

<h2 id="support-for-hugging-face-inference-api-in-weaviate">Support for Hugging Face Inference API in Weaviate</h2>
Starting from Weaviate `v1.15`, Weaviate includes a Hugging Face module, which provides support for Hugging Face Inference straight from the vector database.

The Hugging Face module, allows you to use the [Hugging Face Inference service](https://huggingface.co/inference-api#pricing) with sentence similarity models, to vectorize and query your data, straight from Weaviate. No need to run the Inference API yourself.

<blockquote>
<p>You can choose between `text2vec-huggingface` (Hugging Face) and `text2vec-openai` (OpenAI) modules to delegate your model inference tasks.<br>
Both modules are enabled by default in the [Weaviate Cloud Services](/pricing).</p>
</blockquote>

<h2 id="overview">Overview</h2>
<img src="./img/hugging-face-module-overview.png" alt="Overview" width="100%" />

The Hugging Face module is quite incredible, for many reasons.

<h3 id="public-models">Public models</h3>
You get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case.

In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face).

<h3 id="private-models">Private models</h3>
If you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate.

<!--- TODO: update with a link to the article once it is ready --->
<!--- *We are working on an article that will guide you on how to create your own model and upload it to Hugging Face.* --->

<h3 id="fully-automated-and-optimized">Fully automated and optimized</h3>
Weaviate manages the whole process for you. From the perspective of writing your code – once you have your schema configuration – you can almost forget that Hugging Face is involved at all.

For example, when you import data into Weaviate, Weaviate will automatically extract the relevant text fields, send them Hugging Face to vectorize, and store the data with the new vectors in the database.

<h3 id="ready-to-use-with-a-minimum-of-fuss">Ready to use with a minimum of fuss</h3>
Every new Weaviate instance created with the [Weaviate Cloud Services](/pricing) has the Hugging Face module enabled out of the box. You don't need to update any configs or anything, it is there ready and waiting.

On the other hand, to use the Hugging Face module in Weaviate open source (`v1.15` or newer), you only need to set `text2vec-huggingface` as the default vectorizer. Like this:




<h2 id="how-to-get-started">How to get started</h2>

<!--- :::note
This article is not meant as a hands-on tutorial.
For more detailed instructions please check the [documentation](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface).
::: --->

The overall process to use a Hugging Face module with Weaviate is fairly straightforward.

<h3 id="recipe-for-using-the-hug
