---
title: An Overview on RAG Evaluation
slug: rag-evaluation
authors: [erika, connor]
date: 2023-11-21
tags: [concepts]
image: ./img/hero.png
description: "Learn about new trends in RAG evaluation and the current state of the art."
---
![hero](img/hero.png)

<!---
Retrieval Augmented Generation (RAG) is picking up steam as one of the most popular applications of Large Language Models and Vector Databases. RAG is the process of augmenting inputs to a Large Language Model (LLM) with context retrieved from a vector database, like [Weaviate](https://weaviate.io/). RAG applications are commonly used for chatbots and question-answering systems. 

Like any engineering system, evaluating performance is crucial to the development of RAG applications. The RAG pipeline is broken down into three components: 1. Indexing, 2. Retrieval, and 3. Generation. RAG Evaluation is tricky because of the series of interacting components and the strain of collecting test data. This article will present an exciting development in using LLMs to produce evaluations and the state of RAG components.
--->

Retrieval Augmented Generation (RAG) is a popular application of Large Language Models (LLMs) and Vector Databases. It involves augmenting inputs to an LLM with context retrieved from a vector database, like Weaviate. RAG is commonly used for chatbots and question-answering systems.

Evaluating the performance of RAG systems is crucial for their development. The RAG pipeline consists of three components: Indexing, Retrieval, and Generation. However, RAG Evaluation can be tricky due to the interacting components and the difficulty of collecting test data. This article will discuss the use of LLMs for RAG evaluations and the current state of RAG components.

<!---
**TL;DR**: We were inspired to write this blog post from our conversation with the creators of [Ragas](https://docs.ragas.io/en/latest/), Jithin James and Shauhul Es on the [77th Weaviate podcast](https://www.youtube.com/watch?v=C-UQwvO8Koc). These new advances in using LLMs to evaluate RAG systems, pioneered by Ragas and ARES, motivated us to reflect on previous metrics and take inventory of the RAG knobs to tune. Our investigation led us to think further about what RAG experiment tracking software may look like. We also further clarify how we distinguish RAG systems from Agent systems and how to evaluate each.
--->

**TL;DR**: This blog post was inspired by a conversation with the creators of Ragas and the use of LLMs for RAG evaluations. We will discuss the knobs to tune in RAG systems and the potential for RAG experiment tracking software. We will also distinguish RAG systems from Agent systems and discuss their evaluation.

Our blog post has 5 major sections:

1. **LLM Evaluations**: New trends in using LLMs for RAG performance evaluations and the scales of Zero-Shot, Few-Shot, and Fine-Tuned LLM Evaluators.
2. **RAG Metrics**: Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other.
3. **RAG: Knobs to Tune**: What decision makes RAG systems perform significantly different from one another.
4. **Orchestrating Tuning**: How to manage tracking experimental configurations of RAG systems.
5. **From RAG to Agent Evaluation**: How to evaluate Agent systems and how they differ from RAG systems.

<!---
## LLM Evaluations
Let’s start with the newest and most exciting component of all this, LLM evaluations! The history of machine learning has been heavily driven by the manual labor of labeling data, such as whether a Yelp review is positive or negative, or whether an article about nutritional supplements is related to the query, “Who is the head coach of the Boston Celtics?”. LLMs are becoming highly effective at data annotation with less manual effort. This is the key **“what’s new”** development accelerating the development of RAG applications.

The most common technique pioneered by frameworks, like [Ragas](https://docs.ragas.io/en/latest/), are Zero-Shot LLM Evaluations. Zero-Shot LLM Evaluation describes prompting a Large Language Model with a prompt template such as: “Please provide a rating on a scale of 1 to 10 of whether these search results are relevant to the query. The query is {query}, the search results are {search_results}”. The visualization below shows how an LLM can be used to evaluate the performance of RAG systems.

![RAG-evaluation](img/rag-eval.png)

There are three major opportunities for tuning Zero-Shot LLM Evaluation: 1. the design of the metrics such as precision, recall, or nDCG, 2. the exact language of these prompts, and 3. the language model used for evaluation, such as GPT-4, Coral, Llama-2, Mistral, and many others. 
At the time of writing this blog post, people are mainly curious on the cost of evaluation using an LLM. Let’s use GPT-4 as an example to see the cost of evaluating 10 search results, assuming 500 tokens per result and 100 tokens for the query and instructions, totaling roughly 6,000 tokens per LLM call to make the napkin math easier. Then assuming a rate of $0.005 per 1K tokens, this would cost $3 to evaluate 100 queries.

The adoption of Zero-Shot LLM Evaluation from frameworks like Ragas is widely spreading. This has led to people questioning whether a Few-Shot LLM Evaluation is necessary. Due to its “good enough” status on the tipping scale, Zero-Shot LLM Evaluation may be all that is needed to be a north star for RAG system tuning. Shown below, the RAGAS score is made up of 4 prompts for Zero-Shot LLMs that evaluate the 2 metrics for generation, **Faithfulness** and **Answer Relevancy**, as well as 2 metrics for retrieval, **Context Precision** and **Context Recall**.

![Rag
