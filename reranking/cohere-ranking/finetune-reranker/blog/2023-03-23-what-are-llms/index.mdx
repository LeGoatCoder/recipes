---
title: How GPT4.0 and other Large Language Models Work
slug: what-are-llms
authors: [zain]
date: 2023-03-23
tags: ['concepts']
image: ./img/hero.png
description: "A gentle introduction to Large Language Models (LLMs) - how they work and what they learn."
---

![LLMs](./img/hero.png)

<!--- The post discusses Large Language Models (LLMs) and their capabilities, with a focus on ChatGPT and GPT-4.0. --->

In this blog we discuss:
- How LLMs can generate magically realistic language when prompted
- Question whether they actually understand the content they‚Äôve read in the way humans understand
- Touch on some of the recent improvements and limitations of LLMs


## ChatGPT: Catching Lightning in a Bottle ‚ö°


When OpenAI launched ChatGPT at the end of 2022, more than one million people had tried the model in just a week and that trend has only continued with monthly active users for the chatbot service reaching over 100 Million, quicker than any service before, as reported by [Reuters](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/) and [Yahoo Finance](https://finance.yahoo.com/news/chatgpt-on-track-to-surpass-100-million-users-faster-than-tiktok-or-instagram-ubs-214423357.html?guccounter=1). Since then, OpenAI has been iteratively improving the underlying model, announcing the release of GPT (Generative Pretrained Transformer) 4.0 last week, with new and improved capabilities. It wouldn‚Äôt be hyperbole to say that Natural Language Processing (NLP) and Generative Large Language Models (LLMs) have taken the world by storm.

![gpt100](./img/ChatGPT-2.png)

<!--- This section provides background information on ChatGPT and its capabilities, as well as its impact on the NLP and LLM communities. --->

Though ChatGPT was not the first AI chatbot that has been released to the public, what really surprised people about this particular service was the breadth and depth of knowledge it had and its ability to articulate that knowledge with human-like responses. Aside from this, the generative aspect of this model is also quite apparent as it can hallucinate situations and dream up vivid details to fill in descriptions when prompted to do so. This gives the chatbot service somewhat of a human-like ‚Äúcreativity‚Äù - which is what adds a whole new dimension of utility for the service, as well as a wow factor to the user experience!

In fact, a huge reason why these models are fascinating is because of their ability to paraphrase and articulate a vast general knowledge base by generating human-like phrases and language. If ChatGPT had simply ‚Äúcopy-pasted‚Äù content from the most relevant source to answer our questions, it would be far less impressive a technology. In the case of language models, their ability to summarize, paraphrase and compress source knowledge into realistic language is much more impressive. Ted Chiang captures this exact concept eloquently in his recent article [ChatGPT Is a Blurry JPEG of the Web](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web).

<!--- This section highlights the unique capabilities of ChatGPT and LLMs, including their ability to generate human-like language and paraphrase information from a vast general knowledge base. --->

In this blog, I‚Äôll attempt to explain the intuition of how these models generate realistic language by providing a gentle introduction to their underlying mechanisms. We‚Äôll keep it very high level so that even if you don‚Äôt have a background in machine learning or the underlying mathematics, you‚Äôll understand what they can and cannot do.


## What Are LLMs? - Simply Explained

![books](./img/books.png)

<!--- This section provides a simple explanation of LLMs and their capabilities. --->

I like to think of large language models as voracious readers üëìüìö: they love to read anything and everything they can get their hands on. From cooking recipes to Tupac rap lyrics, from car engine manuals to Shakespeare poetry, from Manga to source code. Anything you can imagine that is publicly available to be read, LLMs have read. Just to give you an idea, GPT 3.0 was trained on 500+ billion words crawled from Wikipedia, literature, and the internet. Think of it this way: LLMs have already read anything you can google and learn. The fact that LLMs have read all of this material is what forms their vast knowledge base.

<!--- This section explains that LLMs are trained on a vast amount of text data, which forms their knowledge base. --->

This explains why LLMs know something about basically everything but it still doesn‚Äôt explain how they articulate this knowledge with human-like responses. The question then arises, what is the point of all this reading!? Are LLMs trying to comprehend what‚Äôs being read? Are they trying to break apart and learn the logic behind the concepts they are reading so they can piece back together concepts on demand upon questioning - as a student preparing for an exam would? Perhaps, but not quite in the way that humans learn and understand from reading. The answer to these questions lies in their name: ‚ÄúLanguage Model‚Äù. **LLMs are trying to build a statistical model of the language they are reading.**

<!--- This section explains that LLMs are trying to build a statistical model of the language they are reading, rather than trying to understand the concepts they are reading in the way that humans do. --->

To understand how LLMs work let's consider a scenario. Imagine you are on the Wheel of Fortune game show and are tasked with completing the following phrase:

![wheel](./img/wheeloffortune.jpeg)

<!--- This section uses a Wheel of Fortune game show scenario to illustrate how LLMs generate human-like responses. --->


The vast majority of you, I bet, would answer: "EASY! It‚Äôs
