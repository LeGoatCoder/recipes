---
title: Multimodal Retrieval Augmented Generation(RAG)
slug: multimodal-RAG
authors: zain
date: 2023-12-05
image: ./img/hero.png
tags: ['concepts', 'how-to']
# tags: ['multimodal-RAG']  # Added for further SEO without changing the original tags
description: "A picture is worth a thousand words, so why just stop at retrieving textual context!? Learn how to perform multimodal RAG!"

---

![The wonderful world of multimodality](./img/hero.png)

The average human hears and learns from about 1 billion words in their entire lifetime. This might be an over-approximation, but it is in the correct ballpark because 1 billion seconds is about 30 years and we don’t hear more than a few words per second. Accounting for sleeping, eating, and other activities; doing some back-of-the-napkin calculations, we can arrive at the [above number](https://youtu.be/Ckz8XA2hW84?t=2286).

The issue, however, is that current Large Language Models(LLMs) are trained on trillions of tokens, many orders of magnitude more data than we ever see in our lifetime, and yet they still don’t have as vivid of an understanding of the causal relationships that exist in the world. From this, we can infer that the way humans learn is fundamentally different from how our current state-of-the-art models learn.

Humans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we can form a coherent understanding of the world, make predictions, acquire new knowledge, and establish causal relationships very efficiently. Not only do humans capture and use multimodal representations of information but given a task we can also incorporate any of these modalities as context to help us guide our answers.

If you’d like to explore this line of thinking further and the potential problems that need to be addressed when getting computers to take advantage of multimodal data, have a look at my previous [blog](https://weaviate.io/blog/multimodal-models) where I cover the topic in detail.

:::info TLDR
In this post we’ll touch on:
- **Contrastive Learning** - One particular approach to training multimodal embedding models that can understand images, audio, video, text, and more
- **Any-to-Any Search and Retrieval** - Using multimodal embedding models to perform any-to-any search and scaling these multimodal embeddings into production using Vector Databases (*With code examples!*)
- **Multimodal Retrieval Augmented Generation(MM-RAG)**: Augmenting the generation from Large Multimodal Models(LMMs) with multimodal retrieval of images and more
- **Code Demo of RAG**
:::

## Contrastive Learning

One way to train a model that understands multimodal data including images, audio, video, and text is to first train individual models that understand each one of these modalities separately and then unify their representations of data using a process called contrastive training.

The idea behind contrastive training is that we can unify the vector space representation of models by taking embeddings across modalities and pushing them further apart or pulling them closer together depending on whether they are similar or different conceptually. This is demonstrated in the image below:

![tripletLoss](./img/image_a.png)
*Source: [Schroff et al. 2015](https://arxiv.org/abs/1503.03832)*

This process was carried out in MetaAI’s [ImageBind paper](https://arxiv.org/abs/2305.05665) to unify vector spaces across 6 different modalities including images, text, audio, and video. To successfully perform contrastive training they used multiple labeled datasets of positive points across multiple modalities and randomly sampled for negative points.

To get a better intuitive understanding of how this process works imagine you embed the image of a lion into vector space using a vision model. The concept behind this object is similar to the audio of a lion roaring, so the audio object embedding can be used as a positive sample and the contrastive loss function works to pull these two points together in embedding space. On the other hand, the embedding of an image of a salad is a negative example and therefore needs to be pushed apart. Have a look at the modification of the above visual to account for cross-modal contrastive training:

![crossMCLR](./img/image_b.png)
*Source: [Zolfaghari et al. 2021](https://lmb.informatik.uni-freiburg.de/Publications/2021/ZB21/)*

If we can continually do this for a large enough dataset of labeled points then we can tighten the representations of data objects in embedding space and even unify the models of different modalities. Another benefit, that ImageBind demonstrated, was that of using frozen image model representations to bind other modalities with cross-modal contrastive loss training - hence the name ImageBind. Embeddings that start differently can then be pulled towards the image representations and thus all of the similar concepts across modalities can be unified such that a concept across all modalities will have similar vectors - demonstrated in the image below. To learn in more depth about contrastive representation learning I would recommend this [blog](https://lilianweng.github.io/posts/2021-05-31-contrastive/).

![unified_emb_Dark](./img/unified_emb_D.png#gh-dark-mode-only)
![unified_emb__Light](./img/unified_emb_L.png#gh-light-mode-only)
*Shows a unified embedding model that captures meanings from any modality that was fused during the contrastive training step.*

## Any-to-Any Search

Once we have a unified embedding space we can take advantage of this to perform cross-modal object operations such as cross-modal search and retrieval, meaning that we can pass in as a query any modality the model understands and use it to
