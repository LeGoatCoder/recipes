---
title: Large Language Models and Search  	<!--- Title of the blog post -->
slug: llms-and-search  	<!--- Unique identifier for the blog post -->
authors: [connor, erika]  	<!--- List of authors for the blog post -->
date: 2023-06-13  	<!--- Date of publication for the blog post -->
image: ./img/hero.png  	<!--- Image to be displayed at the top of the blog post -->
tags: ['search', 'concepts']  	<!--- List of tags associated with the blog post -->
description: "Learn about the intersection between LLMs and Search"  	<!--- Description of the blog post -->

---

![LLMs and Search](./img/hero.png)  	<!--- Image displayed at the top of the blog post -->

<!-- truncate -->

<h2>Introduction</h2>  	<!--- Heading for the introduction section -->

The recent breakthroughs in Large Language Model (LLM) technology are positioned to transition many areas of software. Search and Database technologies particularly have an interesting entanglement with LLMs. There are cases where Search improves the capabilities of LLMs as well as where inversely, LLMs improve the capabilities of Search. In this blog post, we will break down 5 key components of the intersection between LLMs and Search.

<h3>Retrieval-Augmented Generation</h3>  	<!--- Heading for the Retrieval-Augmented Generation section -->

Since the inception of large language models (LLMs), developers and researchers have tested the capabilities of combining information retrieval and text generation. By augmenting the LLM with a search engine, we no longer need to fine-tune the LLM to reason about our particular data. This method is known as [Retrieval-augmented Generation](https://arxiv.org/abs/2005.11401) (RAG).

<h4>How LLMs send Search Queries</h4>  	<!--- Heading for the subsection on how LLMs send search queries -->

Let’s dig a little deeper into how prompts are transformed into search queries. The traditional approach is simply to use the prompt itself as a query. With the innovations in semantic vector search, we can send less formulated queries like this to search engines. For example, we can find information about “the eiffel tower” by searching for “landmarks in France”.

<h3>Query Understanding</h3>  	<!--- Heading for the Query Understanding section -->

Rather than blindly sending a query to the search engine, [Vector Databases](https://weaviate.io/blog/what-is-a-vector-database), such as Weaviate, come with many levers to pull to achieve better search results. Firstly, a query may be better suited for a symbolic aggregation, rather than a semantic search. For example: `“What is the average age of country music singers in the United States of America?”`. This query is better answered with the SQL query: `SELECT avg(age) FROM singers WHERE genre=”country” AND origin=”United States of America”`.

<h3>Index Construction</h3>  	<!--- Heading for the Index Construction section -->

Large Language Models can also completely change the way we index data for search engines, resulting in better search quality down the line. There are 4 key areas to consider here:

1. Summarization indexes
2. Extracting structured data
3. Text chunking
4. Managing document updates.

<h3>LLMs in Re-Ranking</h3>  	<!--- Heading for the LLMs in Re-Ranking section -->

Search typically operates in pipelines, a common term to describe directed acyclic graph (DAG) computation. As discussed previously, `Query Understanding` is the first step in this pipeline, typically followed by retrieval and re-ranking.

<h3>Search Result Compression</h3>  	<!--- Heading for the Search Result Compression section -->

Traditionally, search results are presented to human users as a long list of relevant websites or passages. This then requires additional effort to sift through the results to find the desired information. How can new technologies reduce this additional effort? We will look at this through the lens of (1) question answering and (2) summarization.

<h3>Generative Feedback Loops</h3>  	<!--- Heading for the Generative Feedback Loops section -->

Generative Feedback Loops is a term we are developing to broadly reference cases where the output of an LLM inference is saved back into the database for future use.

<h3>Commentary on LLM Frameworks</h3>  	<!--- Heading for the Commentary on LLM Frameworks section -->

LLM Frameworks, such as [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/) and [LangChain](https://python.langchain.com/en/latest/), serve to control the chaos of emerging LLM applications with common abstractions across projects.

<div class="what-next">
  <h3>What's Next?</h3>  	<!--- Heading for the What's Next section -->
  <p>
    Want to learn more about the intersection of LLMs and Search? Check out our other resources to continue your journey!
  </p>
  <ul>
    <li><a href="/resources/">Blog Posts</a></li>  	<!--- Link to blog posts -->
    <li><a href="/webinars/">Webinars</a></li>  	<!--- Link to webinars -->
    <li><a href="/courses/">Courses</a></li>  	<!--- Link to courses -->
  </ul>
</div>  	<!--- Container for the What's Next section -->

