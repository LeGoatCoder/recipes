---
title: The Details Behind the Sphere Dataset in Weaviate
slug: details-behind-the-sphere-dataset-in-weaviate
authors: [zain]
date: 2022-12-27
tags: ['engineering', 'concepts']
image: ./img/hero.png
description: "Learn about the hardware, software and performance metric specifications behind our ~1B object import of the Sphere dataset into Weaviate."
---

![Sphere dataset in Weaviate](./img/hero.png)

# Introduction
This blog post provides details about how the Sphere dataset was imported into Weaviate. It includes information about the hardware and software setup, as well as performance metrics.

# Background
A previous blog post discussed importing the Sphere dataset into Weaviate and provided a guide on how to do it using Python and Spark. This post goes into more detail about the process.

# Hardware Setup
To set up the cluster, 6 Node GKE's (Google Kubernetes Engine) were provisioned in the US Central1 Region (Zone C). Each node was an instance of type m1-ultramem-40, which has 40 vCPU's and 961 GiB of RAM. In total, the setup had 240 vCPUs and 5766 GiB of memory. Memory optimized instances were chosen because the Compute optimized ones did not have enough memory.

# Software Setup

## Weaviate
6 instances of Weaviate were deployed (one on each GKE node) with automatic sharding per node for a total of 6 shards. Each Weaviate instance had 1800 GiB of disk memory and the `text2vec-huggingface` vectorizer module enabled. The Weaviate version used was built from v1.16.0 with a preview of the Hybrid Search functionality.

## Spark
The Spark Connector was used to import the dataset into Weaviate. It can efficiently populate Weaviate from a Spark dataframe and automatically infer the correct datatype for the Weaviate class schema based on the Spark DataType.

# Importing the Sphere Dataset

The Sphere dataset was imported into a Spark dataframe by loading the Parquet files that were made available. Then, a Weaviate instance was spun up (or a cluster was connected to), a schema for the Sphere dataset was created, and the contents of the Spark dataframe were imported into Weaviate.

# Performance Metrics

The entire import process took approximately 48 hours. The number of vectors imported over time was almost linear, indicating that there was no slow down in the import times regardless of the quantity of objects already imported. The LSM store size was well into terabytes of data, and the inverted index powered BM25+filter searches.

# Batch Import Performance

22 parallel batches each with a batch size of 100 were used for the import. The average batch duration was around 1s, and the batch latency was generally low, keeping under a second for the most part.

# Conclusion

This post provided details about the hardware setup, the usage of the Spark connector, and system performance during the Sphere dataset import run.

<StayConnected />
