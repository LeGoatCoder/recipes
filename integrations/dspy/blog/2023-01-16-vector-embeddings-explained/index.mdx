---
title: Vector Embeddings Explained
slug: vector-embeddings-explained
authors: [dan]
date: 2023-01-16
tags: ['concepts']
image: ./img/hero.png
description: "Get an intuitive understanding of what exactly vector embeddings are, how they're generated, and how they're used in semantic search."
---

The core function of Weaviate is to provide high-quality search results, going beyond simple keyword or synonym searches, and actually finding what the user _means_ by the query, or providing an actual answer to questions the user asks.

<!-- truncate -->

Semantic searches (as well as question answering) are essentially searches by similarity, such as by the meaning of text, or by what objects are contained in images. For example, consider a library of wine names and descriptions, one of which mentioning that the wine is “good with **fish**”. A “wine for **seafood**” keyword search, or even a synonym search, won’t find that wine. A meaning-based search should understand that “fish” is similar to “seafood”, and “good with X” means the wine is “for X”—and should find the wine.

![vector embeddings example](./img/vector-embeddings-example.png)

**How do computers mimic our understanding of language, and similarities of words or paragraphs?** To tackle this problem, semantic search uses at its core a data structure called **vector embedding** (or simply, **vector** or **embedding**), which is an array of numbers. Here's how the semantic search above works, step by step:

1. The [vector database](/blog/vector-library-vs-vector-database) computes a vector embedding for each data object as it is inserted or updated into the database, using a given model.
2. The embeddings are placed into an index, so that the database can [quickly](/blog/why-is-vector-search-so-fast) perform searches.
3. For each query,
    1. a vector embedding is computed using the same model that was used for the data objects.
    2. using a special algorithm, the database find the [closest](/blog/distance-metrics-in-vector-search) vectors to the given vector computed for the query.

**The quality of the search depends crucially on the quality of the model** - this is the "secret sauce", as many models are [still closed source](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The speed of the search depends crucially on Weaviate, which is open-source and [continuously improving its performance](/blog/weaviate-1-18-release).

## What exactly are vector embeddings?

**Vectors are numeric representations of data that capture certain features of the data.** For example, in the case of text data, “cat” and “kitty” have similar meaning, even though the _words_ “cat” and “kitty” are very different if compared letter by letter. For semantic search to work effectively, representations of “cat” and “kitty” must sufficiently capture their semantic similarity. This is where vector representations are used, and why their derivation is so important.

In practice, vectors are arrays of real numbers, of a fixed length (typically from hundreds to thousands of elements), generated by machine learning models. The process of generating a vector for a data object is called vectorization. Weaviate generates vector embeddings using [modules](/developers/weaviate/modules/retriever-vectorizer-modules) (OpenAI, Cohere, Google PaLM etc.), and conveniently stores both objects and vectors in the same database. For example, vectorizing the two words above might result in:




These two vectors have a very high similarity. In contrast, vectors for “banjo” or “comedy” would not be very similar to either of these vectors. To this extent, vectors capture the semantic similarity of words.

Now that you’ve seen what vectors are, and that they can represent meaning to some extent, you might have further questions. For one, what does each number represent? That depends on the machine learning model that generated the vectors, and isn’t necessarily clear, at least in terms of our human conception of language and meaning. But we can sometimes gain a rough idea by correlating vectors to words with which we are familiar.

Vector-based representation of meaning caused quite a [stir](https://www.ed.ac.uk/informatics/news-events/stories/2019/king-man-woman-queen-the-hidden-algebraic-struct) a few years back, with the revelation of mathematical operations between words. Perhaps _the_ most famous result was that of

    “king − man + woman ≈ queen”

It indicated that the difference between “king” and “man” was some sort of “royalty”, which was analogously and mathematically applicable to “queen” minus “woman”. Jay Alamar provided a helpful [visualization](https://jalammar.github.io/illustrated-word2vec/) around this equation. Several concepts (“woman”, “girl”, “boy” etc.) are vectorized into (represented by) an array of 50 numbers generated using the [GloVe model](https://en.wikipedia.org/wiki/GloVe). In [vector terminology](https://en.wikipedia.org/wiki/Vector_(mathematics)), the 50 numbers are called dimensions. The vectors are visualized using colors and arranged next to each word:

![vector embeddings visualization](./img/vector-embeddings-visualization.png)
*Credit: Jay Alamar*

We can see that all words share a dark blue column in one of the dimensions (though we can’t quite tell what that represents), and the word “water” _looks_ quite different from the rest, which makes sense given that the rest are people. Also, “girl” and “boy” look more similar to each other than to “king”
