---
title: An Overview on RAG Evaluation
slug: rag-evaluation
authors: [erika, connor]
date: 2023-11-21
tags: [concepts]
image: ./img/hero.png
description: "Learn about new trends in RAG evaluation and the current state of the art."
---
![hero](img/hero.png)

<!---
Retrieval Augmented Generation (RAG) is picking up steam as one of the most popular applications of Large Language Models and Vector Databases. RAG is the process of augmenting inputs to a Large Language Model (LLM) with context retrieved from a vector database, like [Weaviate](https://weaviate.io/). RAG applications are commonly used for chatbots and question-answering systems. 

Like any engineering system, evaluating performance is crucial to the development of RAG applications. The RAG pipeline is broken down into three components: 1. Indexing, 2. Retrieval, and 3. Generation. RAG Evaluation is tricky because of the series of interacting components and the strain of collecting test data. This article will present an exciting development in using LLMs to produce evaluations and the state of RAG components.
--->

Retrieval Augmented Generation (RAG) is a popular application of Large Language Models (LLMs) and Vector Databases. It involves augmenting inputs to an LLM with context retrieved from a vector database, like Weaviate. RAG is commonly used for chatbots and question-answering systems. Evaluating the performance of RAG systems is crucial and can be broken down into three components: Indexing, Retrieval, and Generation. However, RAG evaluation is tricky due to the interacting components and the difficulty of collecting test data. This article will discuss the use of LLMs for RAG evaluations and provide an overview of the current state of RAG components.

<!---
**TL;DR**: We were inspired to write this blog post from our conversation with the creators of [Ragas](https://docs.ragas.io/en/latest/), Jithin James and Shauhul Es on the [77th Weaviate podcast](https://www.youtube.com/watch?v=C-UQwvO8Koc). These new advances in using LLMs to evaluate RAG systems, pioneered by Ragas and ARES, motivated us to reflect on previous metrics and take inventory of the RAG knobs to tune. Our investigation led us to think further about what RAG experiment tracking software may look like. We also further clarify how we distinguish RAG systems from Agent systems and how to evaluate each.
--->

**TL;DR**: This blog post was inspired by a conversation with the creators of Ragas and the use of LLMs for RAG evaluations. The post discusses previous metrics, RAG knobs to tune, and potential RAG experiment tracking software. It also clarifies the distinction between RAG and Agent systems and their evaluation methods.

The post is divided into five major sections:

1. LLM Evaluations
2. RAG Metrics
3. RAG: Knobs to Tune
4. Orchestrating Tuning
5. From RAG to Agent Evaluation

<!---
Our blog post has 5 major sections:
* [**LLM Evaluations**](#llm-evaluations): New trends in using LLMs to score RAG performance and scales of Zero-Shot, Few-Shot, and Fine-Tuned LLM Evaluators.
* [**RAG Metrics**](#rag-metrics): Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other.
* [**RAG: Knobs to Tune**](#rag-knobs-to-tune): What decision makes RAG systems perform significantly different from one another?
* [**Orchestrating Tuning**](#tuning-orchestration): How do we manage tracking experimental configurations of RAG systems?
* [**From RAG to Agent Evaluation**](#from-rag-to-agent-evaluation): We define RAG as a three step procss of index, retrieve, and generate. This section describes when a RAG system becomes an Agent system and how Agent Evaluation differs.
--->

1. LLM Evaluations

The use of LLMs for RAG evaluations is a new and exciting development. LLMs can be used for zero-shot, few-shot, and fine-tuned evaluations.

<!---
Let’s start with the newest and most exciting component of all this, LLM evaluations! The history of machine learning has been heavily driven by the manual labor of labeling data, such as whether a Yelp review is positive or negative, or whether an article about nutritional supplements is related to the query, “Who is the head coach of the Boston Celtics?”. LLMs are becoming highly effective at data annotation with less manual effort. This is the key **“what’s new”** development accelerating the development of RAG applications.

The most common technique pioneered by frameworks, like [Ragas](https://docs.ragas.io/en/latest/), are Zero-Shot LLM Evaluations. Zero-Shot LLM Evaluation describes prompting a Large Language Model with a prompt template such as: “Please provide a rating on a scale of 1 to 10 of whether these search results are relevant to the query. The query is {query}, the search results are {search_results}”. The visualization below shows how an LLM can be used to evaluate the performance of RAG systems.

![RAG-evaluation](img/rag-eval.png)

There are three major opportunities for tuning Zero-Shot LLM Evaluation: 1. the design of the metrics such as precision, recall, or nDCG, 2. the exact language of these prompts, and 3. the language model used for evaluation, such as GPT-4, Coral, Llama-2, Mistral, and many others. 
At the time of writing this blog post, people are mainly curious on the cost of evaluation using an LLM. Let’s use GPT-4 as an example to see the cost of evaluating 10 search results, assuming 500 tokens per result and 100 tokens for the query and instructions, totaling roughly 6,000 tokens per LLM call to make the napkin math easier. Then assuming a rate of $0.005 per 
