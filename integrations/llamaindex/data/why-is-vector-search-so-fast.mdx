# Title: Why is Vector Search so fast?

## Why is this so incredibly fast?

# Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search.
# To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia.

# For example, I can query Weaviate for articles related to: "urban planning in Europe",
# and the vector database (in the case of my demo â€“ [Weaviate](<https://developers.weaviate.io/>)) responds with a series of articles about the topic, such as "The cities designed to be capitals".

# You can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp),
# which is already pre-populated with the above question. Press the play button, to see the magic happen.

# Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive),
# but it is the ðŸš€ speed at which it all happens. It takes a fraction of a second for the UI to show the results.

# We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**.
# Interestingly enough, it takes longer to render the results than it takes the vector database to find the answer.

# *Note, a semantic search is unlike a regular keywords search (which matches keywords like-for-like),
# but instead, we are searching for answers based on the semantic meaning of our query and data.*

# The inevitable question that follows up this demonstration is always:

> Why is this so incredibly fast?

## What is a vector search?
# To answer this question we need to look at how vector databases work.

# Vector databases index data, unlike other databases, based on data vectors (or vector embeddings).
# Vector embeddings capture the meaning and context of data, usually predicted by Machine Learning models.

# At the time of entry/import (or any significant changes to data objects),
# for every new/updated data object, vector databases use Machine Learning models to predict and calculate vector embeddings,
# which then are stored together with the object.

# > Every data object in a dataset gets a vector

# In a nutshell, vector embeddings are an array of numbers, which can be used as coordinates in a high-dimensional space.
# Although it is hard to imagine coordinates in more than 3-dimensional space (x, y, z),
# we can still use the vectors to compute the distance between vectors, which can be used to indicate similarity between objects. <br/>

# There are many different distance metrics, like [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)
# and [Euclidean distance (L2 distance)](https://en.wikipedia.org/wiki/Euclidean_distance).

### The search part
# In a similar fashion, whenever we run a query (like: "What is the tallest building in Berlin?"),
# a vector database can also convert it to a "query" vector.
# The task of a vector database is to identify and retrieve a list of vectors that are closest to the given vector of your query,
# using a distance metric and a search algorithm.

# This is a bit like a game of boules â€“ where the small marker (jack) is the location of our query vector,
# and the balls (boules) are our data vectors â€“ and we need to find the boules that are nearest to the marker.

## k-nearest neighbors (kNN)
# One way to find similar vectors is with a simple [k-nearest neighbors (kNN) algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm),
# which returns the k nearest vectors, by comparing every data vector in the database to the query vector.

# In our boules example (as illustraded below),
# with 6 boules, the kNN algorithm would measure the distance between the jack and each of the 6 boules on the ground.
# This would result in 6 separate calaculations.

<img
  src={require('./img/knn-boules.png').default}
  alt="kNN search in a game of Boules"
  style={{ maxWidth: "50%" }}
/>

*[Figure 1 - kNN search in a game of Boules.]*

### A kNN search is computationally very expensive
# Comparing a search vector with 10, 100, or 1000 data vectors in just two dimensions is an easy job.
# But of course, in the real world, we are more likely to deal with millions (like in the Wikipedia dataset)
# or even billions of data items.
# In addition, the number of dimensions that most ML models use in semantic search goes up to hundreds or thousands of dimensions!

# The *brute* force of a **kNN search is computationally very expensive** - and depending on the size of your database,
# a single query could take anything from several seconds to even hours (yikesðŸ˜…).
# If you compare a vector with 300 dimensions with 10M vectors, the vector search would need to do 300 x 10M = 3B computations!
# The number of required calculations increases linearly with the number of data points (O(n)) (Figure 2).

<img
  src={require('./img/knn-linear.png').default}
  alt="kNN - O(n) complexity"
  style={{ maxWidth: "50%" }}
/>

*[Figure 2 - O(n) and O(log n) complexity]*

# In summary, kNN search doesn't scale well, and it is hard to image using it with a large dataset in production.

## The answer - Approximate nearest neighbors (ANN)
# Instead of comparing vectors one by one, vector databases use [Approximate Nearest Neighbor (ANN) algorithms](https://en.wikipedia.org/wiki/Nearest_neighbor_search#Approximation_methods),
# which trade off a bit of accuracy (hence the A in the name) for a huge gain
