# Title: LLMs and Search

"""
The recent breakthroughs in Large Language Model (LLM) technology are
positioned to transition many areas of software. Search and Database
technologies particularly have an interesting entanglement with LLMs.
There are cases where Search improves the capabilities of LLMs as well
as where inversely, LLMs improve the capabilities of Search. In this
blog post, we will break down 5 key components of the intersection
between LLMs and Search.
"""

# Retrieval-Augmented Generation
# ==============================

"""
Since the inception of large language models (LLMs), developers and
researchers have tested the capabilities of combining information
retrieval and text generation. By augmenting the LLM with a search
engine, we no longer need to fine-tune the LLM to reason about our
particular data. This method is known as Retrieval-augmented Generation
(RAG).
"""

# Query Understanding
# ===================

"""
Rather than blindly sending a query to the search engine, Vector
Databases, such as Weaviate, come with many levers to pull to achieve
better search results. Firstly, a query may be better suited for a
symbolic aggregation, rather than a semantic search. In order to
bridge this gap, LLMs can be prompted to translate these kinds of
questions into SQL statements which can then be executed against a
database.
"""

# Index Construction
# ==================

"""
Large Language Models can also completely change the way we index
data for search engines, resulting in better search quality down the
line. There are 4 key areas to consider here: 
1. Summarization indexes
2. Extracting structured data
3. Text chunking
4. Managing document updates.
"""

# LLMs in Re-Ranking
# =================

"""
Search typically operates in pipelines, a common term to describe
directed acyclic graph (DAG) computation. As discussed previously,
`Query Understanding` is the first step in this pipeline, typically
followed by retrieval and re-ranking.
"""

# Search Result Compression
# =========================

"""
Traditionally, search results are presented to human users as a long
list of relevant websites or passages. This then requires additional
effort to sift through the results to find the desired information.
How can new technologies reduce this additional effort? We will look at
this through the lens of (1) question answering and (2) summarization.
"""

# Generative Feedback Loops
# =========================

"""
In [our first blog post on Generative Feedback Loops](https://weaviate.io/blog/generative-feedback-loops-with-llms),
we shared an example on how to generate personalized ads for AirBnB
listings. It takes the users preferences and finds relevant listings
that would be of interest to them. The language model then generates an
ad for the listings and links it back to the database with a
cross-reference.
"""

# Commentary on LLM Frameworks
# ===========================

"""
LLM Frameworks, such as [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)
and [LangChain](https://python.langchain.com/en/latest/), serve to
control the chaos of emerging LLM applications with common
abstractions across projects.
"""
