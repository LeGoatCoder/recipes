# What are Distance Metrics in Vector Search?

# Vector databases, like Weaviate, use Machine Learning models to analyze data and calculate vector embeddings.
# The vector embeddings are stored together with the data in a database, and later are used to query the data.

# A vector embedding is an array of numbers that describes an object. For example, strawberries could have a vector
# [3, 0, 1] – more likely the array would be a lot longer than that. The meaning of each value in the array depends
# on what Machine Learning model we use to generate them.

# In order to judge how similar two objects are, we can compare their vector values, by using various Distance Metrics.

# In the context of a Vector Search, a Distance Metric is a function that takes two vectors as input and calculates
# a distance value between the vectors. The distance can take many shapes, it can be the geometric distance between
# two points, it could be an angle between the vectors, it could be a count of vector component differences, etc.
# Ultimately, we use the calculated distance to judge how close or far apart two vector embeddings are.

# Distance Metrics convey how similar or dissimilar two vector embeddings are.

# In this article, we explore the variety of distance metrics (Cosine, Dot product, Euclidean, Manhattan, and Hamming),
# the idea behind each, how they are calculated, and how they compare to each other.

# Background
# ==========

# Vectors in Multi-Dimensional Space
# ----------------------------------

# Vector databases keep the semantic meaning of your data by representing each object as a vector embedding.
# Each embedding is a point in a high-dimensional space. For example, the vector for bananas (both the text and the
# image) is located near apples and not cats.

# Vector Databases are Fast
# -------------------------

# The best thing is, vector databases can query large datasets, containing tens or hundreds of millions of objects
# and still respond to queries in a tiny fraction of a second.

# Without getting too much into details, one of the big reasons why vector databases are so fast is because they
# use the Approximate Nearest Neighbor (ANN) algorithm to index data based on vectors. ANN algorithms organize
# indexes so that the vectors that are closely related are stored next to each other.

# Why are there different distance metrics?
# ------------------------------------------

# Depending on the Machine Learning model used, vectors can have ~100 dimensions or go into thousands of dimensions.

# The time it takes to calculate the distance between two vectors grows based on the number of vector dimensions.
# Furthermore, some distance metrics are more compute-heavy than others. That might be a challenge for calculating
# distances between vectors with thousands of dimensions.

# For that reason, we have different distance metrics that balance the speed and accuracy of calculating distances
# between vectors.

# Cosine distance
# ----------------

# The cosine similarity measures the angle between two vectors in a multi-dimensional space – with the idea that
# similar vectors point in a similar direction. Cosine similarity is commonly used in Natural Language Processing
# (NLP). It measures the similarity between documents regardless of the magnitude. This is advantageous because if
# two documents are far apart by the euclidean distance, the angle between them could still be small.

# The cosine similarity and cosine distance have an inverse relationship. As the distance between two vectors
# increases, the similarity will decrease. Likewise, if the distance decreases, then the similarity between the two
# vectors increases.

# The cosine similarity is calculated as:

# A·B is the product (dot) of the vectors A and B
# ||A||  and ||B|| is the length of the two vectors
# ||A|| * ||B|| is the cross product of the two vectors

# The cosine distance is then calculated as: 1 - Cosine Similarity

# Let's use an example to calculate the similarity between two fruits – strawberries (vector A) and blueberries (vector B).
# Since our data is already represented as a vector, we can calculate the distance.

# Strawberry → `[4, 0, 1]`
# Blueberry →  `[3, 0, 1]`

# A distance of 0 indicates that the vectors are identical, whereas a distance of 2 represents opposite vectors.
# The similarity between the two vectors is 0.998 and the distance is 0.002. This means that strawberries and blueberries
# are closely related.

# Cosine versus Dot Product
# -------------------------

# To calculate the cosine distance, you need to use the dot product. Likewise, the dot product uses the cosine distance
# to get the angle of the two vectors. You might wonder what the difference is between these two metrics.
# The cosine distance tells you the angle, whereas the dot product reports the angle and magnitude.
# If you normalize your data, the magnitude is no longer observable. Thus, if your data is normalized, the cosine and
# dot product metrics are exactly the same.

# Dot Product distance
# --------------------

# The dot product takes two or more vectors and multiplies them together. It is also known as the scalar product
# since the output is a single (scalar) value. The dot product is a similarity metric and shows the direction of two
# vectors. The output value can be any real number. If the vectors are in different directions, then the dot product
# will be negative. Similarly, if the vectors are in the same direction, then the dot product is positive.

# The dot product is calculated as:

# A · B = (A[0]*B[0]) + (A[1]*B[1]) + ... + (A[n]*B[n])

# We will use the same example from above to calculate the dot product of the two vectors.

# Strawberry →  `[4, 0, 1]`
# Blueberry →  `[3, 0, 1]`

# A · B = (4*3) + (0*0) + (1*1) = 13

# The dot product of the two vectors is 13. This indicates that the vectors are similar to one another.
# To calculate the distance, you need to take the negative dot product. The negative dot product reports the

