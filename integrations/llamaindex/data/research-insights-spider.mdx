# Research Insights - Learning to Retrieve Passages without Supervision

## Intro

# There is a lot of excitement around Deep Learning systems with new ML models being trained every day. However, they have been limited by the need for large labeled datasets, which leads us on a search for other solutions that could help us avoid that limitation.

# We found interesting ideas in the [Learning to Retrieve Passages without Supervision](https://arxiv.org/pdf/2112.07708.pdf) paper by Ori Ram, Gal Shachaf, Omer Levy, Jonathan Berant and Amir Globerson. The authors explore
# Self-Supervised Learning – a class of learning algorithms that can bootstrap loss functions – as an alternative route to training new models **without** the need for human labeling!

# Self-Supervised Learning has led to massive advances in generative modeling and representation learning. In this article, we will explain **Span-based Unsupervised Dense Retriever (Spider)**, a recent breakthrough for Self-Supervised representation learning applied to text retrieval in search.

### Results

# The results Spider achieves are extremely exciting! The headline is that Spider's Zero-Shot Generalization rivals the performance of the Supervised baseline on Supervised evaluation. This means that we take the labeled dataset, divide it into train-test splits, train a Supervised model on this train split, and then evaluate both models on the held-out test split. Although Spider has not been trained on this distribution (Zero-Shot Generalization), it achieves a similar performance to the model which has!

# The authors further probe this Zero-Shot Generalization across several Open-Domain Question Answering datasets to illustrate how much better these Self-Supervised algorithms are than their Supervised predecessors. Further, the authors show how we can target Spider’s performance to a particular data distribution with Transfer Learning. The new algorithms prove to be much more sample-efficient, requiring only 128 labeled examples!

## Information Retrieval with Deep Learning

# Information Retrieval describes the task of matching a query with relevant information in a large collection of documents. For example, the question "What is the atomic number of oxygen?" would be paired with the Wikipedia sentence "Oxygen is the chemical element with the symbol O and atomic number 8".

# In order to answer queries based on data from Wikipedia, we take every sentence, or paragraph from Wikipedia, and use a Deep Learning model to build a predicted vector representation. Before we can use the Deep Learning model for this, we need to first train it! And in order to train a Deep Learning model, we need:
# * a dataset (such as paragraphs from Wikipedia),
# * and an optimization task.

# Here we can use Contrastive Learning, which is a strong optimization task. Contrastive Learning uses a combination of positive (semantically similar – we want these to match) and negative (semantically dissimilar – we don't want these to match) samples from a given anchor/query point.

# The image below illustrates the concept of Contrastive Learning. We want to align representations of Meerkat images by contrasting them with sampled negatives. If we choose negatives that are too easy (like comparing Meerkats with a New York City photograph), the model won't learn a whole lot about semantics. However, if we choose a better negative (like a Golden Retriever), the model has to capture more semantics about the visual features of Meerkats.

# <!-- TODO: update the image from Svitlana -->

# ![Bad Negative vs Good Negative](./img/contrastive-learning-example.jpg)
# *Images sourced from Unsplash – Thanks to Bastian Riccardi, Dan Dennis, Clay Banks, and Shayna Douglas!*

# The problem with Contrastive Learning is that the construction of positive and negative samples typically requires expensive and time-consuming manual labeling. Self-Supervised Learning, on the other hand, aims to minimize this cost and offers a clear breakthrough in the performance of these techniques.

# Some quick terms before diving in:
# * Anchor - A query point that the model takes as input to compare with the Positive and Negative.
# * Positive - The data point we want the model to predict is semantically similar to the Anchor.
# * Negative - The data point we want the model to predict is semantically dissimilar to the Anchor.
# * Document - A collection of Passages such as a Wikipedia article, blog post, or scientific paper, to give a few examples.
# * Passage - 100 word chunks extracted from a Document.
# * Span - A snippet of words such as "Deep Learning" or "biological brain of most living organisms".
# * Recurring Span - A piece of text (ignoring punctuation, etc) that appears in more than one Passage.

## What’s New - Span-based Unsupervised Dense Retriever (SPIDER)

# Spider offers a way to automatically generate **Anchor**, **Positive** and **Negative** training data with **Recurring Span Retrieval**. One of the key elements of Spider is to identify the longest Recurring Spans that appear in two (or more) different places within the same Document. Then use that to later construct positive and negative training data.

### Exercise – Spider implementation

# As an exercise, I've prepared a [Google Colab notebook](https://colab.research.google.com/github/CShorten/Small-Weaviate-Examples/blob/main/Recurring-Spans.ipynb), where I implemented a basic version of the Spider Algorithm. The idea is to identify the longest Recurring Spans in the "Deep learning" Wikipedia article.

### Exercise – pseudocode

# In case you are curious on how my implementation works.
# I can explain it in 4 steps:

# * **Step 1** – Split the **Document** in a list of 100 word **Passages**.<br/>
# 	Note. We could also eliminate punctuation and stop words.

# * **Step 2** – Extract **spans (n-grams)** from all **Passages**.<br/>
# 	For example, from the sentence: **"Deep learning (also known as deep structured learning)..."**<br/>
# 	we could get the following 3-grams / spans:<br/>
# 	`["
