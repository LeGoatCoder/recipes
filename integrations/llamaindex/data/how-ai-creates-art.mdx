<!-- A blog post about diffusion models and their use in creating art -->

# How A.I. Creates Art - A Gentle Introduction to Diffusion Models

One of the major developments this past year were the advancements made in machine learning models that can create beautiful and novel images...

...

![Perception of the world](./img/perception_of_the_world.jpg)

...

<!-- Introduction to diffusion models and their capabilities -->

Models like [DALL·E 2](https://openai.com/product/dall-e-2), [Stable Diffusion](https://github.com/Stability-AI/stablediffusion) and others which are the technologies underlying many platforms such as [Lensa](https://prisma-ai.com/lensa) and [Midjourney](http://midjourney.com) are being used by millions of people and are quickly becoming main stream as people realize their potential.

These models not only have the ability to dream up photo-realistic images when prompted with text input but can also modify given images to add details, replace objects or even paint in a given artists style. See below for the Mona Lisa drip painted in the style of Jackson Pollock!

![Mona lisa drip painting](./img/the_mona_lisa_drip_painted.jpg)

...

<!-- Explanation of how diffusion models work -->

## How Diffusion Models Work

Diffusion models are a type of generative model - which means that they can generate data points that are similar to the data points they’ve been trained on(the training set). So when we ask Stable Diffusion to create an image it starts to dream up images that are similar to the billions of images from the internet that it was trained on - it’s important to note that it doesn’t simply copy an image from the training set(which would be no fun!) but rather creates a new image that is similar to the training set. 

Generative models are not limited to just generating images; they can also generate songs, written language or any other modality of data - however to make it easier for us to understand we will only consider generative models that for image data.

The core idea behind all generative models is that they try to learn and understand what the training set “looks” like. In other words they try to learn the underlying distribution of the training set - which just means that they want to know how likely a datapoint is to be observed in the training set. For example, if you are training a generative model on images of beautiful landscapes then, for that generative model, images of trees and mountains are going to be much more common then images of someones kitchen. Furthering this line of reasoning, for that same generative model, an image of static noise would also be quite unlikely since we don’t see that in the training set. This seems like a weird point to make but it will be very important later on!

...

<!-- Explanation of the noising process in diffusion models -->

This “noising” process, shown in the images above allows us to take training set images and add known quantities of noise to it until it becomes completely random noise. This process takes images from a state of having high probability of being found in the training set to having a low probability of existing in the training set.

Once the “noising” step is completed, then we can use these clean and noisy image combinations during the training phase of the diffusion model. In order to train a diffusion model we ask it to remove the noise from the noised images step by step until it recovers something as close as possible to the original image. This process is known as “de-noising”, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added.

...

<!-- Explanation of how text prompts are used to control the image generation process -->

However, another important detail is that most diffusion models don’t just spit out random images that look like training set images, they allow us to add a text prompt that can control the specific types of images are generated. The idea behind conditioning the images on a text prompt requires another model, one that is trained on images along with their captions.

...

<!-- List of resources for generating images using diffusion models -->

Here are some diffusion models that you can use to generate images:

- [Stable Diffusion on Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4)
- A [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb) that uses the same Stable Diffusion model as above if you’d like to tinker with the code.
- [Midjourney](https://midjourney.com/) - allows you to create images by submitting prompts as discord messages
- A [Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) for Stable Diffusion

...

<!-- Suggestions for further reading -->

Here I’ve attempted to provide a general intuition of how diffusion models work, there are many many more details that are involved in this process. If you enjoyed this introduction to diffusion models and would like to dive in deeper to get a better understanding of the code and algorithms involved I would recommend the following blog posts and courses in the following order or increasing complexity:

- [The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/)
- [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)
- [The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion)

<WhatNext />

