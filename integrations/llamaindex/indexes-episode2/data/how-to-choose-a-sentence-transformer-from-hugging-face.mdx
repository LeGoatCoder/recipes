/**
 * How to choose a Sentence Transformer from Hugging Face
 * -----------------------------------------------------
 *
 * Weaviate has recently unveiled a new module which allows users to easily
 * integrate models from Hugging Face to vectorize their data and incoming
 * queries. At the time of this writing, there are over 700 models that can
 * be easily plugged into Weaviate.
 *
 * You may ask: **Why are there so many models and how do they differ?**
 * And more importantly: **How to choose a Sentence Transformer for Semantic
 * Search?**
 *
 * There are too many models to summarize in one flowchart. So instead, we
 * will describe factors that differentiate these models and give you tools
 * to **choose the perfect model for your use case**.
 */

/**
 * Differences in Deep Learning models
 * ------------------------------------
 *
 * Not too long ago, Deep Learning models were typically differentiated based
 * on architectural decisions. For example, ResNet differs from DenseNet based
 * on how frequently they implement skip connections between layers. Fast
 * forward to today, the Deep Learning community has fallen in love with
 * attention layers and the transformer network architecture. Transformers
 * mostly differ between encoder, decoder, and encoder-decoder designs. They
 * additionally vary at the level of details such as the number of layers and
 * hidden dimension sizes. However, the consideration of these kinds of
 * details is mostly a thing of the past thanks to the beauty of the Hugging
 * Face transformers library and the success of this particular model
 * architecture. These details can generally be summarized into the "parameter
 * count" metric, of which most sentence transformers contain about 22
 * million parameters.
 *
 * These days, neural nets differ from each other more so based on **what**
 * they were trained on with respect to **data** -- as well as some subtleties
 * with **how** they were trained, which we will save for a future article.
 * This article will focus on **4 key dimensions**:
 *
 * 1. [Domain](#domain)
 * 2. [Task](#task)
 * 3. [Scale](#scale)
 * 4. [Modality](#modality)
 */

/**
 * Domain
 * ------
 *
 * **Domain** largely describes the high-level notion of what the dataset
 * is about. This is a common term in Machine Learning research to describe
 * differences in large collections of objects. For example, in Computer
 * Vision one **Domain** could be Paintbrush illustrations, and another
 * **Domain** could be Photorealistic images. Both Paintbrush illustrations
 * and Photorealistic images may have been labeled for the same **Task**, i.e.
 * classify images as cats or dogs. The model trained to classify cats in the
 * Paintbrush illustration will not perform as well as the model trained to
 * classify cats in Photorealistic images, if the final use case is
 * Photorealistic images!
 *
 * Domain differences are very common in Natural Language Processing (NLP),
 * such as the difference between Legal Contracts, Financial Statements,
 * Biomedical Scientific Papers, Wikipedia, or Reddit Conversations to give
 * a few examples.
 */

/**
 * Color-coded details
 * -------------------
 *
 * For every model, Hugging Face displays a list of important **color-coded**
 * details, such as:
 *
 * - Blue - the **dataset** it was trained on
 * - Green - the **language** of the dataset
 * - White or Purple - **additional details** about the model
 *
 * So, if we look at two Deep Learning models, we can see that
 * [dangvantuan/sentence-camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large)
 * was trained on **stsb_multi_mt**, which is a **French** dataset.
 *
 * ![Camembert Hugging Face Model Card](./img/camembert-model-card-min.png)
 *
 * While
 * [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)
 * was trained on **several datasets** in **English**.
 *
 * ![all-MiniLM Hugging Face Model Card](./img/minilm-model-card-min.png)
 *
 * To put it as bluntly as possible, what makes `dangvantuan/sentence-camembert-large`
 * better at French sentence embeddings than `sentence-transformers/all-MiniLM-L6-v2`
 * is that… it was trained on **French** sentences! There are many examples like
 * this, models trained on **biomedical text**, **legal documents**, or **Spanish**
 * are generally going to perform better when tested on that domain compared to
 * models that haven't been explicitly trained for the domain.
 *
 * Note that these tags are a part of Hugging Face’s **model cards**, an impressive
 * effort to continue advancing the organization of Machine Learning models. At
 * the time of this writing, model cards still rely on **manual tagging**. It may
 * be the case that the developer uploading the model hasn’t filled out these
 * details. If you are new to Hugging Face, please consider annotating your uploaded
 * models by [adding a model card](https://huggingface.co/docs/hub/model-cards) -
 * YAML sections in the `README.md`, like this:
 *
 * <img
 *     src={require('./img/how-to-populate-model-card-min.png').default}
 *     alt="How to populate a Hugging Face model card"
 *     style={{ maxWidth: "70%" }}
 * />
 */

/**
 * Private models
 * --------------
 *
 * A large part of the beauty of Weaviate's integration with Hugging Face
 * is that **anyone** can upload their models to Hugging Face and use them in
 * Weaviate's vector database. For example, I am doing research on COVID-19
 * literature, so
