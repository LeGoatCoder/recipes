# Title: Vamana vs. HNSW - Exploring ANN algorithms Part 1

# Vector databases must be able to search through a vast number of vectors at speed.
# This is a huge technical challenge that is only becoming more difficult over time
# as the vector dimensions and dataset sizes increase.

# Like many others, our current prevailing solution is to use Approximate Nearest
# Neighbor (ANN) algorithms to power Weaviate. But the key question is - which
# ones to use? There are many different ANN algorithms, each with different
# advantages and limitations.

# Large Scale
# When we talk about a vast number of objects, today we often see use cases with
# hundreds of millions of vectors, but it won't take long until billions, or even
# trillions, will be a pretty standard use case.

# To get vector databases to that kind of scale, we need to constantly evolve and
# look for more efficient solutions. A big part of this search is to explore ANN
# algorithms that would let us go beyond the available RAM (which is a bit of a
# bottleneck) without sacrificing the performance and the UX.

# What to expect
# In this series of blog posts, we will take you on a journey with us as we research
# and implement new ANN algorithms in our quest to reach the 1T goal.

# In this article, we will cover the need for disk-based solutions, explore Vamana
# (how it works and contrast it against HNSW), and present the result of Vamana
# implementation.

# If you are new to vector databases and ANN, I recommend you to read
# "[Why is Vector Search so Fast?](/blog/why-is-vector-search-so-fast)"
# The article explains what vector databases are and how they work.

# Need for approximation
# In order for a vector database to efficiently search through a vast number of
# vectors, the database needs to be smart about it. A brute-force approach would
# calculate the distance between the query vector and every data vector in the
# queried collection, but this is computationally very expensive. For a database
# with millions of objects and thousands of high-dimensional vectors, this would
# take far too long.

# [Weaviate](/developers/weaviate/), an open-source vector database written in Go,
# can serve thousands of queries per second. Running Weaviate on [Sift1M](https://www.tensorflow.org/datasets/catalog/sift1m)
# (a 128-dimensional representation of objects) lets you serve queries in single-digit
# milliseconds. But how is this possible?

# ![SIFT1M Benchmark example](./img/SIFT1M-benchmark.png)
# *See the [benchmark](/developers/weaviate/benchmarks/ann) page for more stats.*

# Weaviate does not look for the exact closest vectors in the store. Instead, it
# looks for approximate (close enough) elements. This means you could have a much
# faster reply, but there is no guarantee that you will actually have the closest
# element from your search. In the vector search space, we use [recall](https://en.wikipedia.org/wiki/Precision_and_recall)
# to measure the rate of the expected matches returned. The trade-off between recall
# and latency can be tuned by adjusting indexing parameters. Weaviate comes with
# reasonable defaults, but also it allows you to adjust build and query-time
# parameters to find the right balance.

# Weaviate incrementally builds up an index (graph representation of the vectors
# and their closest neighbors) with each incoming object. Then when a query arrives,
# Weaviate traverses the index to obtain a good approximated answer to the query
# in a fraction of the time that a brute-force approach would take.

# [HNSW](/developers/weaviate/concepts/vector-index#hnsw) is the first production-ready
# indexing algorithm we implemented in Weaviate. It is a robust and fast algorithm
# that builds a hierarchical representation of the index **in memory** that could
# be quickly traversed to find the k nearest neighbors of a query vector.

# Need for disk solutions
# There are other challenges to overcome. Databases have grown so fast that even
# the above-described algorithms will not be enough. We no longer talk about thousands
# of vectors but **hundred of millions** or **even billions**! Keeping all the
# vectors in memory and adding a graph representation of vector connections **requires
# a lot of RAM**. This sparked the emergence of a new set of algorithms that allow
# vectors to reside on disk instead of in memory whilst retaining high performance.

# Some prominent examples of disk-based solutions include [DiskANN](https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf)
# and [SPANN](https://arxiv.org/abs/2111.08566).

# The future of Weaviate
# Today, users use Weaviate in production to serve large-scale use cases with single-digit
# millisecond latencies and massive throughput. But not every use case requires such
# a high throughput that the cost of keeping all indexes in memory is justified.
# What if you have a case where you have a large-scale dataset but very few queries?
# What if cost-effectiveness is more of a priority than the lowest possible latencies?

# We believe there is a need for other index types besides the battle-tested HNSW
# implementation. But cost-effectiveness can never justify sacrificing the user
# experience. As a result, we are working on establishing a new type of vector index
# that combines the low operating cost of SSD-based solutions with the ease of use
# of existing in-memory solutions.

# At Weaviate, we pride ourselves on our research acumen and on providing state-of-the-art
# solutions. So we took time to explore these solutions to identify and evaluate
# the right building blocks for Weaviate's future.  Here we share some of our findings
# from this research.

# On the HNS
