// GOMEMLIMIT is a game changer for high-memory applications

// Intro
// Running out of memory is never fun, but it's incredibly frustrating when you've already taken some precautions and calculated your exact memory needs. "My application requires 4GB of memory. How is it possible I ran out of memory on my 6GB machine!?". As it turns out, this was a real possibility in a garbage collected ("GC")  language like Golang. The emphasis is on the word "was" because Go 1.19 changes everything: The new `GOMEMLIMIT` feature can help you both increase GC-related performance as well as avoid GC-related out-of-memory ("OOM") situations.

// In this article, we will invite you on a journey. We will cover:
// * How memory allocations work in Go.
// * When the Garbage Collector runs, and what the implications are.
// * Why it was far too easy to run out of memory before Go 1.19.
// * What GOMEMLIMT is and why it can prevent premature OOM kills.
// * Run an experiment first without and then with GOMEMLIMIT

// What is a garbage-collected language?
// In a garbage-collected language, such as Go, C#, or Java, the programmer doesn't have to deallocate objects manually after using them. A GC cycle runs periodically to collect memory no longer needed and ensure it can be assigned again. Using a garbage-collected language is a trade-off between development complexity and execution time. Some CPU time has to be spent at runtime to run the GC cycles.

// Go's Garbage collector is highly concurrent and [quite efficient](https://tip.golang.org/doc/gc-guide#Understanding_costs). That makes Go a great compromise between both worlds. The syntax is straightforward, and the potential for memory-related bugs (such as memory leaks) is low. At the same time, it's not uncommon to have Go programs spend considerably less than 1% of their CPU time on GC-related activities. Given how little the average program is optimized for execution efficiency, trading in just 1% of execution time is a pretty good deal. After all, who wants to worry about freeing memory after it's been used?

// However, as the next few paragraphs show, there are some caveats (and a terrific new solution called GOMEMLIMIT). If you aren't careful, you can run OOM even when you shouldn't. But before we dive into this, we need to talk about stack and heap allocations and why something ends up on the heap.

// The stack vs. the heap
// In short, there are two ways to allocate memory: On the stack or the heap. A stack allocation is short-lived and typically very cheap. No Garbage Collection is required for a stack allocation as the end of the function is also the end of the variable's lifetime. On the other hand, a heap allocation is long-lived and considerably more expensive. When allocating onto the heap, the runtime must find a contiguous piece of memory where the new variable fits. Additionally, it must be garbage-collected when the variable is no longer used. Both operations are orders of magnitudes more expensive than a stack allocation.

// Let's look at two simple examples. The first is a function that takes in two numbers, squares them, and returns the sum of the squares:

func SumSquares(a, b float64) float64 {
  aSquared := a * a // This variable is allocated on the stack
  bSquared := b * b // This variable is also allocated on the stack
  return aSquared + bSquared
}

// Admittedly, this function is a bit more verbose than it would have to be. That is on purpose to show a lot of variables that can live on the stack. There are four variables (a, b, aSquared, and bSquared). None of them "escape" outside of this function block. As a result, the Go runtime can allocate them on the stack. In other words, these allocations are cheap. The garbage collector will never know about them.

// Now, let's look at something that escapes onto the heap. An example application would be a cache. A cache is long-lived and needs to stick around – even after a specific function that interacts with the cache has returned. For example:

var cache = map[string]string{} // This variable is allocated on the heap

func Put(key, value string) {
  cache[key] = value
}

// In the above example, the cache variable is allocated on the heap. It exists before Put is called and after Put has returned. That is by no means the only criterion for why something escapes to the heap, but it should be enough for our purposes of understanding GC-related OOM situations.

// When things accidentally escape to the heap
// The previous examples have shown two distinct cases: Short-lived allocations which end on the stack and long-lived allocations which end up on the heap. In reality, it's not always this simple. Sometimes you will end up with unintentional heap allocations. You allocate a variable that you believe should be short-lived, yet it is allocated on the heap anyway. Why and how that happens is a blog post on its own. It is one of my favorite topics about Go memory management, and I'd be happy to write this post. Please let me know. For this one, it's enough to understand that sometimes heap-allocations happen even when we think they shouldn't. That is important to know because those allocations will put pressure on the GC, which is required for an unexpected OOM situation.

// Why would you run OOM – even when there is enough memory available?
// In the previous sections, we outlined that most applications have short-lived and long-lived memory. Long-lived memory is something you can estimate upfront or control at runtime. For example, if you wanted to extend the simple cache example above to a full-blown cache application, you would most likely implement some sort of a limit. Either the cache would stop accepting new values when it's full or start dropping old ones. You could make sure that the cache never exceeds 4GB in size. Then you should be safe on your 6GB machine, right? The answer is maybe. But "maybe" is not enough when the risk is of running out of memory.

// To understand why it is possible to run OOM in this situation, we need to look at when the Gar
