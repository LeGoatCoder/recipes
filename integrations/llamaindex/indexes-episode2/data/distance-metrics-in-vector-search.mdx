# Vector databases use Machine Learning models to analyze data and calculate vector embeddings.
# Vector embeddings are stored together with the data in a database and later used to query the data.
# A vector embedding is an array of numbers that describes an object.
# In order to judge how similar two objects are, we can compare their vector values using various Distance Metrics.

# A Distance Metric is a function that takes two vectors as input and calculates a distance value between the vectors.
# The distance can take many shapes, such as the geometric distance between two points, an angle between the vectors,
# or a count of vector component differences.

# In this article, we will explore various distance metrics, the idea behind each, how they are calculated,
# and how they compare to each other.

# Background
# Vectors in Multi-Dimensional Space
# Vector databases keep the semantic meaning of your data by representing each object as a vector embedding.
# Each embedding is a point in a high-dimensional space. For example, the vector for bananas (both the text and the image)
# is located near apples and not cats.

# Vector Databases are Fast
# The best thing is, vector databases can query large datasets containing tens or hundreds of millions of objects
# and still respond to queries in a tiny fraction of a second.

# Why are there different distance metrics?
# Depending on the Machine Learning model used, vectors can have ~100 dimensions or go into thousands of dimensions.
# The time it takes to calculate the distance between two vectors grows based on the number of vector dimensions.
# Furthermore, some distance metrics are more compute-heavy than others. That might be a challenge for calculating distances
# between vectors with thousands of dimensions. For that reason, we have different distance metrics that balance the speed
# and accuracy of calculating distances between vectors.

# Cosine Distance
# The cosine similarity measures the angle between two vectors in a multi-dimensional space.
# Cosine similarity is commonly used in Natural Language Processing (NLP) to measure the similarity between documents
# regardless of the magnitude. This is advantageous because if two documents are far apart by the euclidean distance,
# the angle between them could still be small.

# The cosine similarity and cosine distance have an inverse relationship. As the distance between two vectors increases,
# the similarity will decrease. Likewise, if the distance decreases, then the similarity between the two vectors increases.

# The cosine similarity is calculated as:
# A·B is the product (dot) of the vectors A and B
# ||A||  and ||B|| is the length of the two vectors
# ||A|| * ||B|| is the cross product of the two vectors

# The cosine distance is then calculated as: 1 - Cosine Similarity

# Let's use an example to calculate the similarity between two fruits – strawberries (vector A) and blueberries (vector B).
# Strawberry → [4, 0, 1]
# Blueberry →  [3, 0, 1]

# Cosine Similarity Example
# The similarity between the two vectors is 0.998 and the distance is 0.002.
# This means that strawberries and blueberries are closely related.

# Cosine versus Dot Product
# To calculate the cosine distance, you need to use the dot product. Likewise, the dot product uses the cosine distance
# to get the angle of the two vectors. You might wonder what the difference is between these two metrics.
# The cosine distance tells you the angle, whereas the dot product reports the angle and magnitude.
# If you normalize your data, the magnitude is no longer observable. Thus, if your data is normalized, the cosine and dot product
# metrics are exactly the same.

# Dot Product distance
# The dot product takes two or more vectors and multiplies them together. It is also known as the scalar product since
# the output is a single (scalar) value. The dot product is a similarity metric and shows the direction of two vectors.
# The output value can be any real number. If the vectors are in different directions, then the dot product will be negative.
# Similarly, if the vectors are in the same direction, then the dot product is positive.

# The dot product is calculated as:
# A · B = (A1*B1) + (A2*B2) + ... + (An*Bn)

# Let's use the same example from above to calculate the dot product of the two vectors.
# Strawberry →  [4, 0, 1]
# Blueberry →  [3, 0, 1]

# Dot Product Example
# The dot product of the two vectors is 13. This indicates that the vectors are similar to one another.

# L2-Squared distance
# The Squared Euclidean (L2-Squared) calculates the distance between two vectors by taking the sum of the squared vector values.
# The distance can be any value between zero and infinity. If the distance is zero, the vectors are identical.
# The larger the distance, the farther apart the vectors are.

# The squared euclidean distance is calculated as:
# L2 = (A1 - B1)^2 + (A2 - B2)^2 + ... + (An - Bn)^2

# L2-Squared Example
# The squared euclidean distance of strawberries [4, 0, 1] and blueberries [3, 0, 1] is equal to 1.

# Manhattan distance
# Manhattan distance, also known as "L1 norm" and "Taxicab Distance", calculates the distance between a pair of vectors.
# The metric is calculated by summing the absolute distance between the components of the two vectors.

# The name comes from the grid layout resembling the streets of Manhattan.
# The city is designed with buildings on every corner and one-way streets.
# If you're trying to go from point A to point B, the shortest path isn't straight through because you cannot drive through buildings.
# The fastest route is one with fewer twists and turns.

# The Manhattan distance is faster to calculate since the values are typically smaller than the Euclidean distance.

# When to use it
# Generally, there is an accuracy vs. speed tradeoff when choosing between the Manhattan and Euclidean distance.
# It is hard to say precisely when the Manhattan distance will be
