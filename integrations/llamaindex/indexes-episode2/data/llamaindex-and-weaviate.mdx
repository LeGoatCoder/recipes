# Import necessary libraries
import weaviate
from llama_index.node_parser import SimpleNodeParser
from llama_index.readers import SimpleDirectoryReader

# Connect to local Weaviate instance
client = weaviate.Client("http://localhost:8080")

# Load blogs using SimpleDirectoryReader
blogs = SimpleDirectoryReader('./data').load_data()

# Chunk up blog posts into nodes
parser = SimpleNodeParser()
nodes = parser.get_nodes_from_documents(blogs)

# Define WeaviateVectorStore and build vector index over this vector store using LlamaIndex
vector_store = WeaviateVectorStore(weaviate_client = client, index_name="BlogPost", text_key="content")
storage_context = StorageContext.from_defaults(vector_store = vector_store)
index = VectorStoreIndex(nodes, storage_context = storage_context)

# Define a query engine on top of the index
query_engine = index.as_query_engine()

# Perform semantic search and response synthesis, and output an answer
response = query_engine.query("What is the intersection between LLMs and search?")
print(response)

