# Title: The Tile Encoder - Exploring ANN algorithms Part 2.2

# In this blog post, we present an alternative to KMeans, the Tile encoder,
# which is a distribution-based encoder. The Tile encoder works very
# similarly to KMeans but it doesn't need to be fit to the data since it
# leverages the fact that we know the underlying distribution of the data
# beforehand.

# Tile Encoder
# ============
#
# KMeans produces a tiling over the full range of values using the
# centroids. Each centroid has a tile associated with it. When a new
# vector has to be encoded, the algorithm checks which tile it belongs to,
# and the vector code is set using the index of the closest found tile.
# When fitting KMeans to our data the generation of the tiles is not
# homogeneous. As a result, we expect more tiles where there is a higher
# density of data. This way the data will be balanced over the centroids.
#
# If we know the distribution of the data in advance, we might be able to
# produce the centroids without fitting the data with KMeans. All we need
# to do is to produce a tiling over the full range of values following the
# distribution of the data.
#
# Letâ€™s say we want to produce a byte code for each dimension. This would
# be similar to using KMeans with a dimension per segment. Instead of
# using KMeans, we could generate the codes using the Cumulative Density
# Function (CDF) of the distribution of the data. The CDF produces values
# from zero to one. We want 256 codes but in general we might want to use
# any amount of codes (so we could use more, or less bits to encode). The
# code then could be calculated as code(x)=CDF(x)*c where c is the amount
# of codes to use.
#
# On the other hand, when we need to decompress the data, we need to
# generate the centroids from the codes. For this we need to use the
# inverse of the CDF function, meaning that we need the Quantile function.
# The real advantage of this approach is that we no longer need to spend a
# long time to fit our data. We could calculate the mean and standard
# deviation (only parameters of the distribution) incrementally as the
# data is added and that is it.
#
# This also opens a good opportunity for easily updating the Product
# Quantization data over time since the whole process is very cheap in
# terms of time. This case would be interesting if for example we would
# have drifting data. If we compress the data and the data starts
# drifting due to some new trends, then the distribution of the data
# changes and the compression data (KMeans centroids and codes generated
# from them) will become outdated. With the Tile encoder we could monitor
# this situation and update data very quickly.

# Centroid Distributions
# ======================
#
# To get a better understanding of the distribution of the centroids, let
# us create some illustrations with the generated centroids.
#
# Fig. 2: Centroids generated by the KMeans and Tile encoders. Both charts
# show the cartesian product of the first two segments. Both encoders were
# fitted using 32 centroids. Above we show the centroids from KMeans.
# Below we show the centroids from Tile.
#
# As we can observe. Both approaches generate similar results. The
# centroids are very dense at the origin of both axes and much more sparse
# as the values grow. It is worth mentioning that a multivariate approach
# would fit the data better than the cartesian product of individually
# built segments. To depict this, we show Figure 3.
#
# Fig. 3: Centroids generated by KMeans on the first segment including the
# first two dimensions. The encoder was fitted using 32 (above) and 256
# (below) centroids. Centroids fit better the distribution of the data as
# opposed to using independent segments for each dimension when using
# enough centroids.
#
# We are yet to extend the tile encoder to the multivariate case.
# Although this is not extremely difficult it still needs some work and
# will be included in the near future. We have decided to publish this
# initial implementation so it can be tested with more data.
#
# Notice that this encoder depends on the fact that the distribution of
# the data is known a priori. This means that you need to provide this
# information beforehand. Currently we support normal and lognormal
# distributions which are very common. If your data follows a different
# distribution, extending the code is very simple so do not hesitate to
# contact us.
#
# Results of the KMeans Vs Tile encoding
# =====================================
#
# In this section we present some results on the comparison of Product
# Quantization using KMeans vs the Tile encoder.
#
# Table 1: Results of Product Quantization using KMeans and Tile encoders.
# We compare the time to calculate distance, the time to fit and encode
# the data and the recall. We only compare it to KMeans with one
# dimension per segment since the tile encoder would only support this
# setting for now. Also, fitting and encoding times were run on 10 cores
# concurrently, while distance calculations are based on single core
# metrics.
#
# |        | Database    | Data size | Fit    | Encode | Time to calculate distance ($ms$) | Recall |
# |--------|-------------|-----------|--------|--------|----------------------------------------|--------|
# | KMeans | Sift        | 1 M       | 3m15s  | 1m30s  | 701                                    | 0.9973 |
# |        | Gist        | 1 M       | 22m52s | 10m10s | 5426                                   | 0.95   |
# |        | DeepImage96 | 1 M       | 2m14s  | 10m17s | 5276                                   | 0.9725 |
# | Tile   | Sift       
