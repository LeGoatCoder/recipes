# Title: LLMs and Search

"""
The recent breakthroughs in Large Language Model (LLM) technology are positioned to transition many areas of software.
Search and Database technologies particularly have an interesting entanglement with LLMs.
There are cases where Search improves the capabilities of LLMs as well as where inversely, LLMs improve the capabilities of Search.
In this blog post, we will break down 5 key components of the intersection between LLMs and Search.
"""

# Retrieval-Augmented Generation
# ==============================

"""
Since the inception of large language models (LLMs), developers and researchers have tested the capabilities
of combining information retrieval and text generation. By augmenting the LLM with a search engine, we no longer
need to fine-tune the LLM to reason about our particular data. This method is known as Retrieval-augmented Generation (RAG).
"""

# Query Understanding
# ===================

"""
Rather than blindly sending a query to the search engine, Vector Databases, such as Weaviate, come with many levers to pull
to achieve better search results. Firstly, a query may be better suited for a symbolic aggregation, rather than a semantic search.
For example: “What is the average age of country music singers in the United States of America?”

In order to bridge this gap, LLMs can be prompted to translate these kinds of questions into SQL statements which can then be
executed against a database. Similarly, Weaviate has an Aggregate API that can be used for these kinds of queries on symbolic metadata
associated with unstructured data.

In addition to the Text-to-SQL topic, there are also knobs to turn with vector search. The first of which being, Which Class do we want
to search through? In Weaviate, we have the option to divide our data into separate Classes in which each Class has a separate Vector Index,
as well as a unique set of symbolic properties. Which brings us to the next decision when vector searching: Which Filters do we want
to add to the search? A great example of this is filtering search results based on price. We don’t just want the items with the most
semantic similarity to the query, but also those that are less than $100.
"""

# Index Construction
# ==================

"""
Large Language Models can also completely change the way we index data for search engines, resulting in better search quality down the line.
There are 4 key areas to consider here:
1. Summarization indexes
2. Extracting structured data
3. Text chunking
4. Managing document updates.
"""

# LLMs in Re-Ranking
# =================

"""
Search typically operates in pipelines, a common term to describe directed acyclic graph (DAG) computation.
As discussed previously, `Query Understanding` is the first step in this pipeline, typically followed by retrieval and re-ranking.

As described in our previous article, re-ranking models are new to the scene of zero-shot generalization.
The story of re-rankers has mostly been tabular user features combined with tabular product or item features,
fed to XGBoost models. This required a significant amount of user data to achieve, which zero-shot generalization may stand to disrupt.

Cross encoders have gained popularity by taking as input a `(query, document)` pair and outputting a high precision relevance score.
This can be easily generalized to recommendation as well, in which the ranker takes as input a `(user description, item description)` pair.

LLM Re-Rankers stand to transform Cross Encoders by taking in more than one document for this high precision relevance calculation.
This enables the transformer LLM to apply attention over the entire list of potential results. This is also heavily related to the concept of AutoCut with LLMs,
this refers to giving the Language Model k search results and prompting it to determine how many of the k search results are relevant enough to either show to a human user,
or pass along the next step of an LLM computation chain.

Returning to the topic of XGBoost symbolic re-ranking, LLM re-rankers are also quite well-positioned to achieve innovations in symbolic re-ranking as well.
For example, we can prompt the language model like this:

Please rank the following search results according to their relevance with the query: {query}.

Please boost relevance based on recency and if the Author is “Connor Shorten”.

Each search result then comes packaged with their associated metadata in a key-value array.
This offers the additional benefit of allowing business practitioners to easily swap out the ranking logic.
This also holds the benefit of dramatically increasing the interpretability of recommendation systems,
since LLMs can easily be prompted to provide an explanation of the ranking in addition to the ranking itself.
"""

# Search Result Compression
# =========================

"""
Traditionally, search results are presented to human users as a long list of relevant websites or passages.
This then requires additional effort to sift through the results to find the desired information.
How can new technologies reduce this additional effort? We will look at this through the lens of (1) question answering and (2) summarization.
"""

# Question Answering
# ------------------

"""
Even prior to the recent breakthroughs in LLMs, extractive question answering models were beginning to innovate on this experience.
Extractive question answering describes the task of classifying the most likely answer span given a question and text passage as input.
Similar to the ability of ChatGPT to reason about your data without costly training on it, many off the shelf extractive question answering models
can generalize quite well to new (question, text passage) pairs not seen during training.

An example of this is shown below:

![QA example](./img/qa-example.png)
[Source](https://rajpurkar.github.io/SQuAD-explorer/)

One benefit of Extractive QA is that there is no chance of Hallucination.
Hallucination, where Machine Learning models make up information in their responses, is impossible with this kind of technique
because it is completely limited by the input context. That’s not to say that it can’t be incorrect in its answer.
However, on the flip side of that is that these models are limited by the context and thus unable to reason about complex questions
and combine parts of answers assembled across the passage. So unless we have a question as
