# Title: Vector Embeddings Explained

# Semantic searches are searches based on the meaning or context of text or images,
# rather than exact keyword matches. For example, a search for "wine for seafood"
# should return a wine described as "good with fish", even though the keywords
# don't match exactly.

# Vector embeddings are a key component of semantic search. They are arrays of
# numbers that represent the meaning or context of a piece of text or an image.
# By comparing the vector embeddings of different pieces of data, a search
# algorithm can determine how similar they are in meaning or context.

# Here's how semantic search using vector embeddings works:

# 1. When a new piece of data is added to the database, a vector embedding is
#    computed for it using a given model.

# 2. The embeddings are placed into an index, so that the database can quickly
#    perform searches.

# 3. For each query,
#    1. a vector embedding is computed using the same model that was used for
#       the data objects.
#    2. using a special algorithm, the database finds the closest vectors to
#       the given vector computed for the query.

# The quality of the search depends crucially on the quality of the model,
# while the speed of the search depends on the performance of the database.

# Vector embeddings can represent the meaning of text, images, and other types
# of data. They are generated using machine learning models, and the process of
# generating a vector for a piece of data is called vectorization.

# The numbers in a vector embedding don't have a specific meaning on their own,
# but they can be correlated to words or concepts to get a rough idea of what
# they represent. For example, the difference between the vectors for "king"
# and "man" might represent "royalty", which can be applied to the vector for
# "queen" to get a result that is mathematically similar.

# Effective vector embeddings can be generated for any kind of data object,
# including text, images, audio, time series data, 3D models, video, and molecules.
# Two objects with similar semantics will have vectors that are "close" to each
# other, i.e. that have a "small" distance between them in vector space.

# Here's an example of how vector embeddings might be used for a text search:

# | Word               | Vector embedding                |
# |--------------------|---------------------------------|
# | `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |
# | `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |
# | `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |
# | `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |
# | `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |
# | `car`              | `[81.6, -72.1, 16, -20.2, 102]` |
# | **Query: `fruit`** | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |

# In this example, the vector for "fruit" is closest to the vectors for "apple"
# and "strawberry", so those would be the top results for a search for "fruit".

# Vector embeddings are generated using machine learning models, and the
# specific method used to generate them can have a big impact on their quality.
# Over the past decade, there have been significant advances in the field of
# natural language processing, and modern models such as BERT are capable of
# generating high-quality vector embeddings that take into account the context
# of each word in a piece of text.

# There are many different ways to generate vector embeddings, and the best
# method to use depends on the specific use case. Some models are better
# suited for certain types of data or tasks, and it's important to choose the
# right model to get the best results.

# Weaviate is a vector database that supports many different vectorizer models
# and vectorizer service providers. This allows it to generate high-quality
# vector embeddings for a wide range of data types and tasks.

# In addition to supporting many different vectorizer models, Weaviate also
# allows you to bring your own vectors if you already have a vectorization
# pipeline available. This gives you the flexibility to use the vectorization
# method that works best for your specific use case.

# Weaviate supports using any Hugging Face models through the `text2vec-huggingface`
# module, so you can choose one of the many sentence transformers published on
# Hugging Face. It also supports other popular vectorization APIs such as OpenAI
# and Cohere through the `text2vec-openai` and `text2vec-cohere` modules.

# No matter which vectorizer model you choose, the core task performed by all
# of these models is the sameâ€”to represent the "meaning" of the original data
# as a set of numbers. And that's why semantic search works so well.
