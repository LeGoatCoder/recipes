# Import necessary libraries
import weaviate
from pyspark.sql import SparkSession

# Initialize a SparkSession
spark = SparkSession.builder.config(
    "spark.jars",
    "gcs-connector-hadoop3-latest.jar,weaviate-spark-connector-assembly-v0.1.2.jar",
).master("local[*]").appName("weaviate").getOrCreate()

# Load the Sphere dataset Parquet files
df = spark.read.parquet("gs://sphere-demo/parquet/sphere.899M.parquet")

# Connect to a local Weaviate instance
client = weaviate.Client("http://localhost:8080")

# Create a schema for the Sphere dataset
client.schema.create_class(
    {
        "class": "Sphere",
        "properties": [
            {"name": "raw", "dataType": ["string"]},
            {"name": "sha", "dataType": ["string"]},
            {"name": "title", "dataType": ["string"]},
            {"name": "url", "dataType": ["string"]},
        ],
    }
)

# Import the contents of the Spark dataframe into Weaviate
df.withColumnRenamed("id", "uuid").write.format("io.weaviate.spark.Weaviate") \
    .option("batchSize", 100) \
    .option("scheme", "http") \
    .option("host", "localhost:8080") \
    .option("id", "uuid") \
    .option("className", "Sphere") \
    .option("vector", "vector") \
    .mode("append").save()

# The entire import process took approximately 48 hours

