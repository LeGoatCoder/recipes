# HNSW+PQ - Exploring ANN algorithms Part 2.1

# Weaviate is already a very performant and robust vector database and with the recent release of  v1.18 we are now bringing vector compression algorithms to Weaviate users everywhere.
# The main goal of this new feature is to offer similar performance at a fraction of the memory requirements and cost.
# In this blog we expand on the details behind this delicate balance between recall performance and memory management.

# In our previous blog [Vamana vs. HNSW - Exploring ANN algorithms Part 1](/blog/ann-algorithms-vamana-vs-hnsw), we explained the challenges and benefits of the Vamana and HNSW indexing algorithms.
# However, we did not explain the underlying motivation of moving vectors to disk. In this post we explore:

# - What kind of information we need to move to disk
# - The challenges of moving data to disk
# - What the implications of moving information to disk are
# - Introduce and test a full first solution offered by the new HNSW+PQ feature in Weaviate v1.18

# ## What Information to Move to disk

# When indexing, there exist two big chunks of information that utilize massive amounts of memory: vectors and neighborhood graphs.

# Weaviate currently supports vectors of `float32`. This means we need to allocate 4 bytes, or 32 bits, per stored vector dimension.
# A database like Sift1M contains 1 million vectors of 128 dimensions each. This means that holding the vectors in memory requires 1,000,000 x 128 x 4 bytes = 512,000,000 bytes.

# Additionally, a graph representation of neighborhoods is built when indexing. The graph represents the k-nearest neighbors for each vector.
# To identify each neighbor we use an `int64`, meaning we need 8 bytes to store each of the k-nearest neighbors per vector.
# The parameter controlling the size of the graph is `maxConnections`. If we use `maxConnections = 64` when indexing Sift1M we end up with 1,000,000 x 64 x 8 bytes = 512,000,000 bytes also for the graph.
# This would bring our total memory requirements to around ~1 GB in order to hold both, the vectors and the graph, for 1,000,000 vectors each with 128 dimensions.

# 1 GB doesn't sound too bad! Why should we go through the trouble of compressing the vectors at all then!?
# Sift1M is a rather small dataset. Have a look at some of the other experimental datasets listed in Table 1 below; these are much bigger and this is where the memory requirements can begin to get out of hand.
# Furthermore, you might be using Weaviate with your own data which might be even bigger than the datasets we report on below.
# If we extrapolate out, consider how big these memory requirements could grow as you add more objects or represent these objects with long vector embeddings.

# | DataSet     | Dimensions | Vectors   | Size in memory (MB) |
# |-------------|------------|-----------|---------------------|
# | Sift1M      | 128        | 1,000,000 | 512                 |
# | Gist        | 960        | 1,000,000 | 3840                |
# | DeepImage96 | 96         | 9,990,000 | 3836.16             |
# **Table 1**: *Description of datasets.*

# Increasing the number of objects vs. storing longer dimensional vectors has an equivalent effect on the overall memory required to store the vectors.
# As an example consider the Gist dataset, which contains 1,000,000 vectors, each with 960 dimensions. This would mean we need roughly 500 MB for the graph but nearly ten times more memory for the vectors.
# On the other hand, a database such as DeepImage96 would have 96 dimensions but almost 10,000,000 vectors, meaning, that we would need around 10 GB to hold the vectors and the graph, ~5 GB for each graph.

# Our final goal is to move both vectors and graphs to disk. However, we will only explore moving vectors to disk in this post.

# Storing vectors on disk is not too challenging. The problem is that moving vectors to disk would have higher latency costs since we would then need lots of disk reads.
# The proposed solution by [DiskANN](https://proceedings.neurips.cc/paper/2019/file/09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf) is to store large complete representations of vectors on disk and keep a compressed representation of them in memory.
# The compressed representation is used to sort the vectors while searching for the nearest neighbors, and the complete representation is fetched from disk every time we need to explore a new vector from the sorted list.

# In plain English, we start our search from our root in the graph. From there, we get a set of neighbor candidates.
# We need to explore each candidate one at a time. We sort the candidates using the compressed representations, stored in memory, and decide what the best next candidate to explore is.
# Exploring a candidate would mean fetching it from disk. Notice now that we are not reading all vectors from disk, but only the most promising candidates we wish to explore.
# This way we still need to host the compressed vectors in memory, but it will give us enough information to reduce disk reads significantly.

# The next natural question is: How do we compress vectors?

# ## How to Efficiently Compress Vectors

# The main idea behind vector compression is to have a “good-enough” representation of the vectors (as opposed to a perfect representation)
# so they take up less space in memory while still allowing us to calculate the distance between them in a performant and accurate way.
# Compression could come from different sources. We could, for example, aim to reduce redundant data to store information more efficiently.
# We could also sacrifice accuracy in the data in favor of space. We aim to do the latter in this post.

# Once we
