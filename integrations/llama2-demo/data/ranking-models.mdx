# Title: Ranking Models for Better Search
# -------------------------------------
#
# This code discusses the importance of ranking models in improving search quality.
# It covers three categories of ranking models: Cross Encoders, Metadata Rankers,
# and Score Rankers.
#
# The code also discusses the use of ranking models in hybrid search, which
# combines contextual semantics from vector search and keyword matching from BM25
# sparse search.
#
# Cross Encoders are content-based re-ranking models that utilize pre-trained
# models to rank the relevance of documents. They offer the advantage of further
# reasoning about the relevance of results without needing specialized training.
#
# Metadata Rankers are context-based re-rankers that use symbolic features to rank
# relevance. They take into account user and document features to predict the
# relevance of candidate documents.
#
# Score Rankers employ classifiers or regression models to score and detect
# content, acting as guardrails for generative models.
#
# Each of these ranking models have particular use cases. However, the lines
# between these models are blurring with new trends such as translating tabular
# metadata features into text to facilitate transfer learning from transformers
# pre-trained on text.
#
# The recent successes of LLMs are causing a rethink of most AI workflows and the
# application of LLMs to rank and score rankers to filter generations are both
# exciting.
#
# The code ends with a discussion on ranking for Retrieval-Augmented Generation,
# which is the use of ranking models in large language models for completing
# complex tasks.

# Cross Encoders
# --------------

# Cross Encoders are content-based re-ranking models that utilize pre-trained
# models, such as those available on Sentence Transformers, to rank the relevance
# of documents.

# They offer the advantage of further reasoning about the relevance of results
# without needing specialized training.

# Cross Encoders can be interfaced with Weaviate to re-rank search results,
# trading off performance for slower search speed.

# Here's an example of a Cross Encoder in action using the Weaviate Podcast Search
# dataset:

# Query: Are there any ways to benchmark the performance of self-ask prompting?

# | Ranker                | Output                                                                                       |
# |---------------------------|----------------------------------------------------------------------------------------------------|
# | Cross Encoder                  | That's a great question. Honestly, you should invite Ofir to chat about that. And it's a great question. I hadn't thought of it that way. I don't know where it is in the data and it may simply be that the model is mashing up all this other data that it has in an interesting way, the same way that you can get stable diffusion to give you like people sitting around a campfire on an airplane, I think I saw go by, or like salmon swimming in a stream where it was literal pieces of salmon. That was never anywhere in the training set, but it managed to mash it up and figure out what to do with it. And I wonder if something similar is happening here. What I love about all this work, like self-ask, chain of thought, we're developing new querying languages. This is like us inventing SQL, except that we didn't design the database. The database came into being and we have to figure out how to interact with it. That example I mentioned about the IPython interaction, like that's a, again, it's a new querying language. And I honestly thought the most potent part of the self-ask wasn't even necessarily the self-ask part. It was that Ofir did such a phenomenal job of figuring out a way to measure the complexity of the knowledge that was extracted from the model. He gave us a benchmark, a ladder to climb, a way to measure whether we could retrieve certain kinds of information from models. And I think that's going to open the door to a ton more benchmarks. And you know what happens when there's a benchmark. We optimize the hell out of that benchmark and it moves science forward… [ truncated for visibility ] |
# | Hybrid Only            | Or, at least being able to ask follow up questions when it’s unclear about and that’s surprisingly not that difficult to do with these current systems, as long as you’re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might get. |

# LLMs as Cross Encoders
# ----------------------

# So, let’s dive into the LLM hype a little more, how can we use LLMs for re-ranking?
# There are generally 2 ways to do this.

# The first strategy is identical to the cross encoder, we give the LLM the [query,
# document] input and prompt it to output a score of how relevant the document is
# to the query.

# The tricky thing with this is bounding the score. One technique is to prompt it
# with: `please output a relevance score on a scale of 1 to 100.`

# I think the second strategy is a bit more interesting, in which we put as many
# documents as we can in the input and ask the LLM to rank them.

# The key to making this work is the emergence of LLMs to follow instructions,
# especially with formatting their output.

# By prompting this ranking with “please output the ranking as a dictionary of
# IDs with the key equal to the rank and the value equal to the document id”.

# Also interesting is the question around how many documents we can rank like this
# and how expensive it is.

# For example, if we want to re-rank 100 documents, but can only fit 5 in the input
# at a time, we are going to need to construct some kind of tournament-style
# decomposition of the ranking task.

# Further these approaches are well positioned to generalize to Recommendation.
# In Recommendation, instead of taking a [query, document] as input to a
# cross-encoder, we take as input a [user description, document] pair.

# There is a bonus 3rd idea where we use the log probabilities concatenating the

