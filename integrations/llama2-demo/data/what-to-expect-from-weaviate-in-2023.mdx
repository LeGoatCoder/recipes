<h1>What to expect from Weaviate in 2023</h1>

<p>In this blog post, I will introduce you to the six pillars outlining how Weaviate will get even better in 2023. Weaviate development is highly dynamic – we don’t waterfall-plan for the entire year – but nevertheless, we want to give you the best possible overview of what to expect in the coming year.</p>

## The Six Pillars for 2023

### Pillar 1: Ingestion and Search Pipelines

<img src="./img/search-pipeline.png" alt="Ingestion and Search Pipelines">

<p>Weaviate’s strong and growing module ecosystem gives you plenty of flexibility. Whether you use Weaviate as a pure vector search engine or with the addition of vectorizer, reader, and generator modules, you can always configure it to your liking. In early 2023 we even saw the addition of the [`generative-openai`](/developers/weaviate/modules/reader-generator-modules/generative-openai) module (with other generative modules to come).</p>

<p>We want to give you even more flexibility in combining these steps this year. You can control arbitrary querying steps through the <a href="https://github.com/weaviate/weaviate/issues/2560">proposed Pipe API</a>, such as reading, re-ranking, summarizing, generating, and others. Similarly, we want to give you more flexibility during ingestion time: how about <a href="https://github.com/weaviate/weaviate/issues/2509">extracting PDFs</a> or applying <a href="https://github.com/weaviate/weaviate/issues/2439">stemming</a> to your BM25 and hybrid search?</p>

<br></br>

### Pillar 2: Beyond Billion Scale: Large-Scale Performance

<img src="./img/billion-scale.png" alt="Billion Scale">

<p>In 2022, we published the <a href="/blog/sphere-dataset-in-weaviate">Sphere Demo Dataset for Weaviate</a>. This marked the first time (to our knowledge) that more than a billion objects and vectors were imported into Weaviate. Dealing with ever-growing datasets is not only about being able to handle their size. Our users run complex queries in production and often have strict latency requirements. This pillar is all about performance. The first big step will be the move towards a <a href="https://github.com/weaviate/weaviate/issues/2511">Native Roaring Bitmap Index</a>.</p>

<p>In the most extreme case, this new index can speed up filtered vector search <a href="https://twitter.com/etiennedi/status/1621180981519458305">by a factor of 1000</a>. But it doesn’t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate.</p>

<br></br>

### Pillar 3: Cloud Operations & Scaling

<img src="./img/cloud-operations-scaling.png" alt="cloud operations scaling">

<p>When we introduced <a href="/developers/weaviate/concepts/replication-architecture">Replication</a> to Weaviate in late 2022, we celebrated a significant milestone. It’s never been easier to achieve a highly available setup, and you can even dynamically scale your setup to increase throughput. 2023 is all about improving your cloud and operations experience.</p>

<p>We will give you more control over <a href="https://github.com/weaviate/weaviate/issues/2586">how to structure your workloads</a> in a distributed setup and more <a href="https://github.com/weaviate/weaviate/issues/2228">flexibility to adapt to your ever-changing needs</a>. And, of course, we’re constantly working on making your distributed cluster <a href="https://github.com/weaviate/weaviate/issues/2405">even more resilient</a>.</p>

<p>Speaking of Cloud, arguably the easiest way to spin up a new use case with Weaviate is through the <a href="/pricing">Weaviate Cloud Services</a>.</p>

<br></br>

### Pillar 4: New Vector Indexes

<img src="./img/vector-indexes.png" alt="vector indexes">

<p>Last year we gave you a sneak peek into our <a href="/blog/ann-algorithms-vamana-vs-hnsw">Vector Indexing Research</a>, and this year you will be able to try out new vector indexes for yourself. Since the beginning, Weaviate has supported vector indexing with [HNSW](/developers/weaviate/concepts/vector-index), which leads to <a href="/developers/weaviate/benchmarks/ann">best-in-class query times</a>. But not every use case requires single-digit millisecond latencies. Instead, some prefer cost-effectiveness.</p>

<p>Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance.</p>

<p>This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time. We are already working on a fully disk-based solution which we will release in late 2023.</p>

<br></br>

### Pillar 5: Improving our Client and Module Ecosystem

<img src="
